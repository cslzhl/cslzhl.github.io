<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Latex使用教程</title>
    <url>/archives/7a335556.html</url>
    <content><![CDATA[<h1 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h1><h2 id="tabincell"><a href="#tabincell" class="headerlink" title="tabincell"></a>tabincell</h2><p>使用方法<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\tabincell&#123;l&#125;&#123;以方法为顶点，两方法\\引用构成的无向图的连通分支&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="multirow和multicol"><a href="#multirow和multicol" class="headerlink" title="multirow和multicol"></a>multirow和multicol</h2><p>\multirow{nrows}[bigstructs]{width}[fixup]{text}<br>nrows   设定所占用的行数。<br>bigstructs  此为可选项，主要是在你使用了 bigstruct 宏包时使用。<br>width  设定该栏文本的宽度。如果想让 LaTeX 自行决定文本的宽度，则用 * 即可。<br>fixup  此为可选项，主要用来调整文本的垂直位置。<br>text    所要排版的文本。可用 \ 来强迫换行。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\begin&#123;tabular&#125;&#123;ccccrc&#125;</span><br><span class="line">  \hline</span><br><span class="line">  \textbf&#123;Datasets&#125; &amp; \textbf&#123;Projects&#125; &amp; \textbf&#123;Level&#125; &amp; \textbf&#123;\#Features&#125; &amp; \textbf&#123;\#Instances&#125; &amp; \textbf&#123;\#Faulty Instances (Percent)&#125; \\\hline %\hline</span><br><span class="line">    %\midrule</span><br><span class="line"></span><br><span class="line">   \multirow&#123;3&#125;&#123;*&#125;&#123;ReLink&#125; &amp; Apache &amp; \multirow&#123;3&#125;&#123;*&#125;&#123;File&#125;  &amp; \multirow&#123;3&#125;&#123;*&#125;&#123;26&#125; &amp; 194 &amp;\ 98 (50.52\%) \\</span><br><span class="line">   &amp; Safe &amp;  &amp;  &amp; 56 &amp;\ 22 (39.29\%) \\</span><br><span class="line">   &amp; ZXing &amp;  &amp;  &amp; 399 &amp; 118 (29.57\%) \\\hline %\hline</span><br><span class="line"></span><br><span class="line">    \multirow&#123;5&#125;&#123;*&#125;&#123;AEEEM&#125; &amp; EQ &amp; \multirow&#123;5&#125;&#123;*&#125;&#123;Class&#125;  &amp; \multirow&#123;5&#125;&#123;*&#125;&#123;61&#125; &amp; 324 &amp; 129 (39.8\%)\ \\</span><br><span class="line">   &amp; JDT &amp;  &amp;  &amp; 997 &amp; 206 (20.7\%) \\</span><br><span class="line">   &amp; LC &amp;  &amp;  &amp; 691 &amp;64 (9.3\%) \\</span><br><span class="line">   &amp; ML &amp;  &amp;  &amp; 1\hspace&#123;0.25ex&#125;862 &amp; 245 (13.2\%) \\</span><br><span class="line">   &amp; PDE &amp;  &amp;  &amp; 1\hspace&#123;0.25ex&#125;497 &amp; 209 (14.0\%) \\\hline</span><br><span class="line">  %\bottomrule</span><br><span class="line">\end&#123;tabular&#125;</span><br></pre></td></tr></table></figure><br>结果图：<br><img src="http://ww1.sinaimg.cn/large/006ltHaXly1gotapwipcsj30pn0ae0ui.jpg" alt="QQ截图20210323033228.png"><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\multicolumn&#123;2&#125;&#123;c&#125;&#123;之前版本&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="表格画虚线"><a href="#表格画虚线" class="headerlink" title="表格画虚线"></a>表格画虚线</h2><p>The arydshln package offers you the \hdashline and \cdashline<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\documentclass&#123;report&#125;</span><br><span class="line">\usepackage&#123;arydshln&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;tabular&#125;&#123;c:cc&#125;</span><br><span class="line">   column1a &amp; column2a &amp; column3a \\</span><br><span class="line">   column1b &amp; column2b   &amp; column3b\\ \hdashline</span><br><span class="line">   column1c &amp; column2c &amp; column3c \\ \cdashline&#123;1-2&#125;</span><br><span class="line">   column1d &amp; column2d &amp; column3d \\</span><br><span class="line">\end&#123;tabular&#125;</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="表格画分开的线段"><a href="#表格画分开的线段" class="headerlink" title="表格画分开的线段"></a>表格画分开的线段</h2><p>使用\cmidrule(r){7-8}</p>
<h2 id="表格设置行间距列间距"><a href="#表格设置行间距列间距" class="headerlink" title="表格设置行间距列间距"></a>表格设置行间距列间距</h2><p>\renewcommand{\arraystretch}{1.3}%调行距<br>\setlength\tabcolsep{3pt}%调列距</p>
<h2 id="表格单元格设置背景颜色"><a href="#表格单元格设置背景颜色" class="headerlink" title="表格单元格设置背景颜色"></a>表格单元格设置背景颜色</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\usepackage&#123;colortbl&#125;</span><br><span class="line"># \rowcolor&#123;颜色&#125;、\columncolor&#123;颜色&#125;、\cellcolor&#123;颜色&#125;</span><br><span class="line">\cellcolor&#123;lightgray&#125;</span><br></pre></td></tr></table></figure>
<h2 id="文字设置背景色"><a href="#文字设置背景色" class="headerlink" title="文字设置背景色"></a>文字设置背景色</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\usepackage&#123;color&#125;%可以用于中文</span><br><span class="line">\colorbox&#123;yellow&#125;&#123;yellow background&#125;</span><br></pre></td></tr></table></figure>
<h2 id="表格单元格排列及长宽设置"><a href="#表格单元格排列及长宽设置" class="headerlink" title="表格单元格排列及长宽设置"></a>表格单元格排列及长宽设置</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\usepackage&#123;array&#125;</span><br><span class="line">\begin&#123;tabular&#125;&#123;cp&#123;23em&#125;m&#123;2em&#125;b&#123;2em&#125;&#125;                              </span><br><span class="line">        \hline                                                     </span><br><span class="line">        横向居中&amp;居下&amp;居中&amp;居上\\                                  </span><br><span class="line">        \hline                                                     </span><br><span class="line">        a b c d e f g&amp; a b c d e f g&amp;a b c d e f g&amp;a b c d e f g\\ </span><br><span class="line">\hline                                                             </span><br><span class="line">\end&#123;tabular&#125;                                                      </span><br></pre></td></tr></table></figure>
<h2 id="表格重设表序号"><a href="#表格重设表序号" class="headerlink" title="表格重设表序号"></a>表格重设表序号</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># \setcounter&#123;page&#125;&#123;3&#125;</span><br><span class="line">\setcounter&#123;table&#125;&#123;1&#125; # 下个表格序号加一</span><br></pre></td></tr></table></figure>
<h1 id="有序列表和无序列表"><a href="#有序列表和无序列表" class="headerlink" title="有序列表和无序列表"></a>有序列表和无序列表</h1><p>有序列表使用方括号标号或自定义标号<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\begin&#123;enumerate&#125;[label&#x3D;&#123;[\arabic*]&#125;]  %\setlength&#123;\itemsep&#125;&#123;0pt&#125;</span><br><span class="line">\item xxxxx   %[1] xxxxx</span><br><span class="line">\item yyyyyy  %[2] yyyyyy</span><br><span class="line">\item zzzzz   %[3] zzzzz</span><br><span class="line">\end&#123;enumerate&#125;</span><br></pre></td></tr></table></figure><br>其中，</p>
<ul>
<li>enumerate环境后面跟的参数label={[\arabic*]}即可使得列表以“方括号+阿拉伯数字”的形式编号；</li>
<li>在\begin{enumerate}[label={[\arabic*]}]后面跟\setlength{\itemsep}{0pt}可以设置当前列表环境里item条目之间的间距。</li>
<li>\arabic可以替换为\roman、\Roman、\Alph 或 \alph来表示小写罗马数字、大写罗马数字、大写字母编号 或 小写字母编号。</li>
</ul>
<h1 id="个人常用命令"><a href="#个人常用命令" class="headerlink" title="个人常用命令"></a>个人常用命令</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\newcommand&#123;\tabincell&#125;[2]&#123;\begin&#123;tabular&#125;&#123;@&#123;&#125;#1@&#123;&#125;&#125;#2\end&#123;tabular&#125;&#125;</span><br><span class="line"># 加红</span><br><span class="line">\newcommand&#123;\cx&#125;[1]&#123;\textcolor&#123;black&#125;&#123;#1&#125;&#125;</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://tex.stackexchange.com/questions/20140/can-a-table-include-a-horizontal-dashed-line">Can a table include a horizontal dashed line?</a></li>
<li><a href="https://blog.csdn.net/moonlightpeng/article/details/88138586">latex longtable and supertabular 跨页表格</a></li>
<li><a href="https://blog.csdn.net/haifeng_gu/article/details/109168733">Latex中设置enumerate有序列表使用方括号标号以及设置item间距</a></li>
<li><a href="https://blog.csdn.net/bleedingfight/article/details/80169003">LaTeX入门学习(5)(表格)</a></li>
<li><a href="https://blog.csdn.net/wangliandehanyun/article/details/9471427">LaTex中表格固定列宽并且居中的方法</a></li>
<li><a href="https://www.jianshu.com/p/9d73201b965b">LaTeX固定表格每一列宽度并指定对齐方式（居中）</a></li>
</ol>
]]></content>
      <categories>
        <category>Latex</category>
      </categories>
      <tags>
        <tag>Latex</tag>
      </tags>
  </entry>
  <entry>
    <title>cs-cloud GPU服务器环境配置</title>
    <url>/archives/175f7e9.html</url>
    <content><![CDATA[<h1 id="配置Docker源"><a href="#配置Docker源" class="headerlink" title="配置Docker源"></a>配置Docker源</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 更新源</span><br><span class="line">$ sudo apt update</span><br><span class="line"></span><br><span class="line"># 启用HTTPS</span><br><span class="line">$ sudo apt install -y \</span><br><span class="line">    apt-transport-https \</span><br><span class="line">    ca-certificates \</span><br><span class="line">    curl \</span><br><span class="line">    gnupg-agent \</span><br><span class="line">    software-properties-common</span><br><span class="line"></span><br><span class="line"># 添加GPG key</span><br><span class="line">$ curl -fsSL https:&#x2F;&#x2F;download.docker.com&#x2F;linux&#x2F;ubuntu&#x2F;gpg | sudo apt-key add -</span><br><span class="line"></span><br><span class="line"># 添加稳定版的源</span><br><span class="line">$ sudo add-apt-repository \</span><br><span class="line">   &quot;deb [arch&#x3D;amd64] https:&#x2F;&#x2F;download.docker.com&#x2F;linux&#x2F;ubuntu \</span><br><span class="line">   $(lsb_release -cs) \</span><br><span class="line">   stable&quot;</span><br></pre></td></tr></table></figure>
<h1 id="安装Docker-CE"><a href="#安装Docker-CE" class="headerlink" title="安装Docker CE"></a>安装Docker CE</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 更新源</span><br><span class="line">$ sudo apt update</span><br><span class="line"></span><br><span class="line"># 安装Docker CE</span><br><span class="line">$ sudo apt install -y docker-ce</span><br></pre></td></tr></table></figure>
<h1 id="验证Docker-CE"><a href="#验证Docker-CE" class="headerlink" title="验证Docker CE"></a>验证Docker CE</h1><p>如果出现下面的内容，说明安装成功。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo docker run hello-world</span><br><span class="line"></span><br><span class="line">Unable to find image &#39;hello-world:latest&#39; locally</span><br><span class="line">latest: Pulling from library&#x2F;hello-world</span><br><span class="line">1b930d010525: Pull complete </span><br><span class="line">Digest: sha256:2557e3c07ed1e38f26e389462d03ed943586f744621577a99efb77324b0fe535</span><br><span class="line">Status: Downloaded newer image for hello-world:latest</span><br><span class="line"></span><br><span class="line">Hello from Docker!</span><br><span class="line">This message shows that your installation appears to be working correctly.</span><br><span class="line"></span><br><span class="line">To generate this message, Docker took the following steps:</span><br><span class="line"> 1. The Docker client contacted the Docker daemon.</span><br><span class="line"> 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub.</span><br><span class="line">    (amd64)</span><br><span class="line"> 3. The Docker daemon created a new container from that image which runs the</span><br><span class="line">    executable that produces the output you are currently reading.</span><br><span class="line"> 4. The Docker daemon streamed that output to the Docker client, which sent it</span><br><span class="line">    to your terminal.</span><br><span class="line"></span><br><span class="line">To try something more ambitious, you can run an Ubuntu container with:</span><br><span class="line"> $ docker run -it ubuntu bash</span><br><span class="line"></span><br><span class="line">Share images, automate workflows, and more with a free Docker ID:</span><br><span class="line"> https:&#x2F;&#x2F;hub.docker.com&#x2F;</span><br><span class="line"></span><br><span class="line">For more examples and ideas, visit:</span><br><span class="line"> https:&#x2F;&#x2F;docs.docker.com&#x2F;get-started&#x2F;</span><br></pre></td></tr></table></figure></p>
<h1 id="配置nvidia-docker源"><a href="#配置nvidia-docker源" class="headerlink" title="配置nvidia-docker源"></a>配置nvidia-docker源</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 添加源</span><br><span class="line">$ distribution&#x3D;$(. &#x2F;etc&#x2F;os-release;echo $ID$VERSION_ID)</span><br><span class="line">$ curl -s -L https:&#x2F;&#x2F;nvidia.github.io&#x2F;nvidia-docker&#x2F;gpgkey | sudo apt-key add -</span><br><span class="line">$ curl -s -L https:&#x2F;&#x2F;nvidia.github.io&#x2F;nvidia-docker&#x2F;$distribution&#x2F;nvidia-docker.list | sudo tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;nvidia-docker.list</span><br><span class="line"></span><br><span class="line"># 安装并重启docker</span><br><span class="line">$ sudo apt update &amp;&amp; sudo apt install -y nvidia-container-toolkit</span><br><span class="line">$ sudo systemctl restart docker</span><br></pre></td></tr></table></figure>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在官方CUDA镜像上测试 nvidia-smi</span><br><span class="line">$ sudo docker run --gpus all nvidia&#x2F;cuda:9.0-base nvidia-smi</span><br><span class="line"></span><br><span class="line"># 启动支持双GPU的容器</span><br><span class="line">$ sudo docker run --gpus 2 nvidia&#x2F;cuda:9.0-base nvidia-smi</span><br><span class="line"></span><br><span class="line"># 指定GPU 1，运行容器</span><br><span class="line">$ sudo docker run --gpus device&#x3D;0 nvidia&#x2F;cuda:9.0-base nvidia-smi</span><br></pre></td></tr></table></figure>
<p>能看到显卡信息就说明OK了，当前image是基于Ubuntu 16.04的。</p>
<h1 id="docker-默认存储目录更改"><a href="#docker-默认存储目录更改" class="headerlink" title="docker 默认存储目录更改"></a>docker 默认存储目录更改</h1><p>查看Docker默认存储目录：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看docker基本信息</span><br><span class="line">$ docker info</span><br><span class="line">...</span><br><span class="line">Docker Root Dir: &#x2F;var&#x2F;lib&#x2F;docker</span><br><span class="line">Debug Mode: false</span><br><span class="line">Registry: https:&#x2F;&#x2F;index.docker.io&#x2F;v1&#x2F;</span><br><span class="line">Labels:</span><br><span class="line">Experimental: false</span><br></pre></td></tr></table></figure><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo systemctl stop docker</span><br><span class="line">$ sudo vim &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;http:&#x2F;&#x2F;hub-mirror.c.163.com&quot;],</span><br><span class="line">  &quot;data-root&quot;: &quot;&#x2F;mnt&#x2F;data&quot; &#x2F;&#x2F;添加存储路径</span><br><span class="line">&#125;</span><br><span class="line">$ sudo systemctl start docker</span><br></pre></td></tr></table></figure></p>
<h1 id="删除镜像和容器"><a href="#删除镜像和容器" class="headerlink" title="删除镜像和容器"></a>删除镜像和容器</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo docker images # 查看所有镜像</span><br><span class="line">sudo docker ps -a # 显示所有容器</span><br><span class="line">sudo docker rm 容器id # 删除容器</span><br><span class="line">sudo docker rmi 镜像id # 删除镜像</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.cnblogs.com/journeyonmyway/p/11234572.html">Ubuntu 18.04安装Docker CE及NVIDIA Container Toolkit流程</a></li>
</ol>
]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>ftx量化空间</title>
    <url>/archives/9060a5db.html</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="02bf989b047b22bf8cfcb5193b8209c30aa94df8583da5de7d4d0fda4e8b2465">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>交易</category>
      </categories>
      <tags>
        <tag>量化</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo主题美化及常用插件使用</title>
    <url>/archives/a6fdd32d.html</url>
    <content><![CDATA[<p>前言：本博客搭建时使用的nodejs版本为12.20.1，hexo版本为5.3.0，next版本为7.4.0，初次搭建推荐做一些文件改动时就测试一下(我到最后才发现hexo d居然出问题，艹)<br><a id="more"></a></p>
<h1 id="站点配置"><a href="#站点配置" class="headerlink" title="站点配置"></a>站点配置</h1><h2 id="修改post初始生成样式"><a href="#修改post初始生成样式" class="headerlink" title="修改post初始生成样式"></a>修改post初始生成样式</h2><p>在目录scaffolds下，修改post.md</p>
<h1 id="主题配置"><a href="#主题配置" class="headerlink" title="主题配置"></a>主题配置</h1><h2 id="fork-me-on-github"><a href="#fork-me-on-github" class="headerlink" title="fork me on github"></a>fork me on github</h2><p>当前版本next集成了这个功能。在<strong>主题配置</strong>中修改即可</p>
<h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># &#96;Follow me on GitHub&#96; banner in the top-right corner.</span><br><span class="line">github_banner:</span><br><span class="line">  enable: true</span><br><span class="line">  permalink: https:&#x2F;&#x2F;github.com&#x2F;cslzhl</span><br><span class="line">  title: Follow me on GitHub</span><br></pre></td></tr></table></figure></h2><p>之前版本在<strong>主题目录</strong>(themes/next/layout)下修改_layout.swig文件，可从<a href="https://tholman.com/github-corners/">GitHub Corners</a>中选一种样式</p>
<figure class="highlight diff"><table><tr><td class="code"><pre><span class="line">&lt;div class=&quot;&#123;&#123; container_class &#125;&#125; &#123;% block page_class %&#125;&#123;% endblock %&#125; &quot;&gt;</span><br><span class="line">&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;</span><br><span class="line"><span class="addition">+ &lt;a href=&quot;https://github.com/cslzhl&quot; class=&quot;github-corner&quot; aria-label=&quot;View source on GitHub&quot;&gt;&lt;svg width=&quot;80&quot; height=&quot;80&quot; viewBox=&quot;0 0 250 250&quot; style=&quot;fill:#64CEAA; color:#fff; position: absolute; top: 0; border: 0; right: 0;&quot; aria-hidden=&quot;true&quot;&gt;&lt;path d=&quot;M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z&quot;&gt;&lt;/path&gt;&lt;path d=&quot;M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2&quot; fill=&quot;currentColor&quot; style=&quot;transform-origin: 130px 106px;&quot; class=&quot;octo-arm&quot;&gt;&lt;/path&gt;&lt;path d=&quot;M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z&quot; fill=&quot;currentColor&quot; class=&quot;octo-body&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;style&gt;.github-corner:hover .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125;@keyframes octocat-wave&#123;0%,100%&#123;transform:rotate(0)&#125;20%,60%&#123;transform:rotate(-25deg)&#125;40%,80%&#123;transform:rotate(10deg)&#125;&#125;@media (max-width:500px)&#123;.github-corner:hover .octo-arm&#123;animation:none&#125;.github-corner .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125;&#125;&lt;/style&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="阅读全文跳转"><a href="#阅读全文跳转" class="headerlink" title="阅读全文跳转"></a>阅读全文跳转</h2><p>修改主题文件(themes/next/layout/_macro/post.swig)</p>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a><figure class="highlight diff"><table><tr><td class="code"><pre><span class="line"><span class="deletion">- &lt;a class=&quot;btn&quot; href=&quot;&#123;&#123; url_for(post.path) &#125;&#125;#more&quot; rel=&quot;contents&quot;&gt;</span></span><br><span class="line"><span class="addition">+ &lt;a class=&quot;btn&quot; href=&quot;&#123;&#123; url_for(post.path) &#125;&#125;&quot; rel=&quot;contents&quot;&gt;</span></span><br></pre></td></tr></table></figure></h2><p><strong>以下方法以失效</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scroll_to_more: false</span><br></pre></td></tr></table></figure></p>
<h2 id="推荐文章插件"><a href="#推荐文章插件" class="headerlink" title="推荐文章插件"></a>推荐文章插件</h2><p>这个帮助我们根据标签推荐相关文章，当前版本NexT主题集成了这个插件的配置。</p>
<h3 id="地址"><a href="#地址" class="headerlink" title="地址"></a>地址</h3><p><a href="https://github.com/tea3/hexo-related-popular-posts">hexo-related-popular-posts</a></p>
<h3 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h3><p>安装插件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-related-popular-posts --save</span><br></pre></td></tr></table></figure><br>修改主题配置<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">related_posts:</span><br><span class="line">  enable: true</span><br><span class="line">  title: 推荐文章 # Custom header, leave empty to use the default one</span><br><span class="line">  display_in_home: false</span><br><span class="line">  params:</span><br><span class="line">    maxCount: 5</span><br><span class="line">    PPMixingRate: 0.25</span><br><span class="line">    isDate: false</span><br><span class="line">    isImage: false</span><br><span class="line">    isExcerpt: false</span><br></pre></td></tr></table></figure></p>
<h2 id="Hexo修改back2top标签"><a href="#Hexo修改back2top标签" class="headerlink" title="Hexo修改back2top标签"></a>Hexo修改back2top标签</h2><h3 id="地址-1"><a href="#地址-1" class="headerlink" title="地址"></a>地址</h3><p><a href="https://github.com/jiangtj-lab/hexo-cake-moon-menu">hexo-cake-moon-menu</a></p>
<h3 id="安装配置-1"><a href="#安装配置-1" class="headerlink" title="安装配置"></a>安装配置</h3><p>安装插件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-cake-moon-menu --save</span><br></pre></td></tr></table></figure><br>修改站点配置<br>在站点配置文件_config.yml 中添加以下代码：</p>
<h2 id="-2"><a href="#-2" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">moon_menu:</span><br><span class="line">  back2top:</span><br><span class="line">    enable: true</span><br><span class="line">    icon: fa fa-chevron-up</span><br><span class="line">    func: back2top</span><br><span class="line">    order: -1</span><br><span class="line">  back2bottom:</span><br><span class="line">    enable: true</span><br><span class="line">    icon: fa fa-chevron-down</span><br><span class="line">    func: back2bottom</span><br><span class="line">    order: -2</span><br></pre></td></tr></table></figure></h2><p>在next主题配置中，以下为back2top条：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">back2top:</span><br><span class="line">  enable: true</span><br><span class="line">  # Back to top in sidebar.</span><br><span class="line">  sidebar: true # 在左边文章目录下</span><br><span class="line">  # Scroll percent label in b2t button.</span><br><span class="line">  scrollpercent: true # 显示百分比</span><br></pre></td></tr></table></figure></p>
<h2 id="侧边栏标签等数字显示"><a href="#侧边栏标签等数字显示" class="headerlink" title="侧边栏标签等数字显示"></a>侧边栏标签等数字显示</h2><p>修改next主题配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">menu_settings:</span><br><span class="line">  icons: true</span><br><span class="line">  badges: true</span><br></pre></td></tr></table></figure>
<h2 id="Hexo文章加密插件"><a href="#Hexo文章加密插件" class="headerlink" title="Hexo文章加密插件"></a>Hexo文章加密插件</h2><h3 id="地址-2"><a href="#地址-2" class="headerlink" title="地址"></a>地址</h3><p><a href="https://github.com/D0n9X1n/hexo-blog-encrypt">hexo-blog-encrypt</a></p>
<h3 id="安装配置-2"><a href="#安装配置-2" class="headerlink" title="安装配置"></a>安装配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-blog-encrypt --save</span><br></pre></td></tr></table></figure>
<p>修改站点配置文件,增加如下代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Security</span><br><span class="line">encrypt: # hexo-blog-encrypt</span><br><span class="line">  abstract: 有东西被加密了, 请输入密码查看.</span><br><span class="line">  message: 您好, 这里需要密码.</span><br><span class="line">  tags:</span><br><span class="line">  - &#123;name: tagName, password: 密码A&#125;</span><br><span class="line">  - &#123;name: tagName, password: 密码B&#125;</span><br><span class="line">  wrong_pass_message: 抱歉, 这个密码看着不太对, 请再试试.</span><br><span class="line">  wrong_hash_message: 抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.</span><br></pre></td></tr></table></figure>
<p>对博文禁用 Tag 加密</p>
<p>只需要将博文头部的 password 设置为 “” 即可取消 Tag 加密.</p>
<h2 id="Hexo-豆瓣读书、豆瓣电影插件"><a href="#Hexo-豆瓣读书、豆瓣电影插件" class="headerlink" title="Hexo 豆瓣读书、豆瓣电影插件"></a>Hexo 豆瓣读书、豆瓣电影插件</h2><h3 id="安装配置-3"><a href="#安装配置-3" class="headerlink" title="安装配置"></a>安装配置</h3><p>安装插件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-douban --save</span><br></pre></td></tr></table></figure>
<p>启动：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo douban -bgm &amp;&amp; hexo g &amp;&amp; hexo s</span><br></pre></td></tr></table></figure>
<h3 id="-3"><a href="#-3" class="headerlink" title=" "></a> </h3><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://tding.top/archives/567debe0.html">本博客当前使用的插件总结</a></li>
</ol>
]]></content>
      <categories>
        <category>建站教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github page</tag>
      </tags>
  </entry>
  <entry>
    <title>python并行处理</title>
    <url>/archives/a448f9a6.html</url>
    <content><![CDATA[<h1 id="基于进程的并行"><a href="#基于进程的并行" class="headerlink" title="基于进程的并行"></a>基于进程的并行</h1><h2 id="方法1-multiprocessing"><a href="#方法1-multiprocessing" class="headerlink" title="方法1: multiprocessing"></a>方法1: multiprocessing</h2><a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="comment"># map与imap,对于很长的迭代对象，可能消耗很多内存。可以考虑使用 imap()</span></span><br><span class="line"><span class="comment"># map(func, iterable[, chunksize])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">twotimes</span>(<span class="params">i</span>):</span></span><br><span class="line">    <span class="keyword">return</span> i * <span class="number">2</span></span><br><span class="line">pool = mp.Pool(processes = mp.cpu_count())</span><br><span class="line">results = pool.<span class="built_in">map</span>(twotimes, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">print(results)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="方法2-Parallel"><a href="#方法2-Parallel" class="headerlink" title="方法2: Parallel"></a>方法2: Parallel</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> joblib <span class="keyword">import</span> Parallel, delayed, cpu_count</span><br><span class="line">batches = Parallel(n_jobs=cpu_count() - <span class="number">1</span>)(</span><br><span class="line">    delayed(extract)(i, br, bug_reports, java_src_dict)</span><br><span class="line">    <span class="keyword">for</span> i, br <span class="keyword">in</span> <span class="built_in">enumerate</span>(bug_reports)</span><br><span class="line">)</span><br><span class="line">features = [row <span class="keyword">for</span> batch <span class="keyword">in</span> batches <span class="keyword">for</span> row <span class="keyword">in</span> batch]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="方法3-pathos-pools"><a href="#方法3-pathos-pools" class="headerlink" title="方法3: pathos.pools"></a>方法3: pathos.pools</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pathos.pools <span class="keyword">as</span> pp</span><br></pre></td></tr></table></figure>
<h2 id="方法4-numba"><a href="#方法4-numba" class="headerlink" title="方法4: numba"></a>方法4: numba</h2><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://docs.python.org/zh-cn/3.8/library/multiprocessing.html">multiprocessing —- 基于进程的并行</a></li>
<li><a href="https://blog.csdn.net/weixin_42001089/article/details/88843152?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.control">python 并行化：加快数据的处理</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/46378282">python并行计算（上）：pathos模块</a></li>
<li><a href="https://www.jianshu.com/p/69d9d7e37bc5">加速python运行-numba</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>并行处理</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>python数据的存储和持久化操作</title>
    <url>/archives/f82593e0.html</url>
    <content><![CDATA[<p>np.save,pickle.dump,json.dump<br><a id="more"></a></p>
<h1 id="异同"><a href="#异同" class="headerlink" title="异同"></a>异同</h1><ol>
<li>jSON 是一个文本序列化格式（它输出 unicode 文本，尽管在大多数时候它会接着以 utf-8 编码），而 pickle 是一个二进制序列化格式；</li>
<li>JSON是可互操作的，在Python系统之外广泛使用，而pickle则是Python专用的；</li>
<li>默认情况下，JSON 只能表示 Python 内置类型的子集，不能表示自定义的类；但 pickle 可以表示大量的 Python 数据类型</li>
</ol>
<h1 id="json"><a href="#json" class="headerlink" title="json"></a>json</h1><h2 id="json-dumps和json-loads"><a href="#json-dumps和json-loads" class="headerlink" title="json.dumps和json.loads"></a>json.dumps和json.loads</h2><p>json.dumps将一个Python数据结构转换为JSON<br>json.loads将一个JSON编码的字符串转换回一个Python数据结构<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># 字典</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">dict</span>=&#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;simon&#x27;</span>,<span class="string">&#x27;age&#x27;</span>:<span class="number">1</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> json</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">dict</span> = json.dumps(<span class="built_in">dict</span>)</span><br><span class="line"><span class="string">&#x27;&#123;&quot;name&quot;: &quot;simon&quot;, &quot;age&quot;: 1&#125;&#x27;</span> <span class="comment">#序列化之后的数据</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">dict</span>  = json.loads(<span class="built_in">dict</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="built_in">dict</span>)</span><br><span class="line">&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;simon&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">1</span>&#125; <span class="comment">#反序列化之后恢复回字典</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">type</span>(<span class="built_in">dict</span>)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">dict</span>&#x27;&gt;</span></span><br><span class="line"><span class="class">&gt;&gt;&gt; # 列表</span></span><br><span class="line">&gt;&gt;&gt; a = [[1,2,3],[4,5]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>alist = json.dumps(a)</span><br><span class="line"><span class="string">&#x27;[[1, 2, 3], [4, 5]]&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="json-dump-和-json-load"><a href="#json-dump-和-json-load" class="headerlink" title="json.dump() 和 json.load()"></a>json.dump() 和 json.load()</h2><p>json.dump() 和 json.load() 来编码和解码JSON数据,用于处理文件<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;test.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    json.dump(data, f)  <span class="comment"># 编码JSON数据</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;test.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = json.load(f)  <span class="comment"># 解码JSON数据</span></span><br></pre></td></tr></table></figure></p>
<h1 id="数组保存"><a href="#数组保存" class="headerlink" title="数组保存"></a>数组保存</h1><h2 id="numpy-savetxt"><a href="#numpy-savetxt" class="headerlink" title="numpy.savetxt"></a>numpy.savetxt</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.random.rand(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">np.savetxt(<span class="string">&#x27;a.txt&#x27;</span>,a,fmt=<span class="string">&#x27;%0.8f&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="numpy-save"><a href="#numpy-save" class="headerlink" title="numpy.save"></a>numpy.save</h2><p>numpy.save可直接保存列表<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">np.save(<span class="string">&quot;number.npy&quot;</span>, a)</span><br><span class="line">k = np.load(<span class="string">&quot;number.npy&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.cnblogs.com/huajiezh/p/5470770.html">python数据的存储和持久化操作</a></li>
<li><a href="https://blog.csdn.net/blances/article/details/95227359">json和pickle</a></li>
<li>[]</li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>存储</tag>
      </tags>
  </entry>
  <entry>
    <title>python执行cmd命令</title>
    <url>/archives/1012268b.html</url>
    <content><![CDATA[<p>我们通常可以使用os模块的命令进行执行cmd</p>
<h2 id="方法1：os-system"><a href="#方法1：os-system" class="headerlink" title="方法1：os.system"></a>方法1：os.system</h2><a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># os.system(执行的指令)</span></span><br><span class="line">code = os.system(<span class="string">&#x27;ls&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="方法2-os-popen"><a href="#方法2-os-popen" class="headerlink" title="方法2: os.popen"></a>方法2: os.popen</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># os.popen(执行的指令)</span></span><br><span class="line">code = os.popen(<span class="string">&#x27;ls&#x27;</span>)</span><br><span class="line">code.read() <span class="keyword">or</span> code.readlines()</span><br></pre></td></tr></table></figure>
<h2 id="两者区别"><a href="#两者区别" class="headerlink" title="两者区别"></a>两者区别</h2><ul>
<li>system返回指令执行的结果，0表示执行成功，反之则为错误代码，这是系统依赖(system-dependent)的。system无法获取指令输出的信息。</li>
<li>popen可以获取输出的信息内容，它是一个文件对象，可以通过 .read() 去读取</li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>cmd</tag>
      </tags>
  </entry>
  <entry>
    <title>ptyhon数组堆叠</title>
    <url>/archives/bcd3a59.html</url>
    <content><![CDATA[<p>常见的方法有torch.cat,torch.stack,np.cat,np.stack<br><a id="more"></a></p>
<h1 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h1><h2 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat"></a>torch.cat</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A=torch.ones(<span class="number">2</span>,<span class="number">3</span>) <span class="comment">#2x3的张量（矩阵）</span></span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B=<span class="number">2</span>*torch.ones(<span class="number">4</span>,<span class="number">3</span>)<span class="comment">#4x3的张量（矩阵）  </span></span><br><span class="line">tensor([[ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span> C=torch.cat((A,B)) <span class="comment"># or C=torch.cat((A,B),0)</span></span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>D=<span class="number">2</span>*torch.ones(<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C=torch.cat((A,D),<span class="number">1</span>)<span class="comment">#按维数1（列）拼接</span></span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack"></a>torch.stack</h2><p>函数stack()对序列数据内部的张量进行<strong>扩维</strong>拼接，指定维度由程序员选择、大小是生成后数据的维度区间。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A=torch.rand(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B=torch.ones(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.stack((A,B),dim=<span class="number">0</span>) <span class="comment"># shape: [2, 3, 3]</span></span><br><span class="line">tensor([[[<span class="number">0.7021</span>, <span class="number">0.8243</span>, <span class="number">0.5045</span>],</span><br><span class="line">         [<span class="number">0.8906</span>, <span class="number">0.0391</span>, <span class="number">0.1455</span>],</span><br><span class="line">         [<span class="number">0.1793</span>, <span class="number">0.5437</span>, <span class="number">0.2779</span>]],</span><br><span class="line">        [[<span class="number">0.8316</span>, <span class="number">0.1665</span>, <span class="number">0.1453</span>],</span><br><span class="line">         [<span class="number">0.1797</span>, <span class="number">0.8958</span>, <span class="number">0.8249</span>],</span><br><span class="line">         [<span class="number">0.3707</span>, <span class="number">0.0720</span>, <span class="number">0.9599</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.stack((A,B),dim=<span class="number">1</span>) <span class="comment"># shape: [3, 2, 3]</span></span><br><span class="line">tensor([[[<span class="number">0.7021</span>, <span class="number">0.8243</span>, <span class="number">0.5045</span>],</span><br><span class="line">         [<span class="number">0.8316</span>, <span class="number">0.1665</span>, <span class="number">0.1453</span>]],</span><br><span class="line">        [[<span class="number">0.8906</span>, <span class="number">0.0391</span>, <span class="number">0.1455</span>],</span><br><span class="line">         [<span class="number">0.1797</span>, <span class="number">0.8958</span>, <span class="number">0.8249</span>]],</span><br><span class="line">        [[<span class="number">0.1793</span>, <span class="number">0.5437</span>, <span class="number">0.2779</span>],</span><br><span class="line">         [<span class="number">0.3707</span>, <span class="number">0.0720</span>, <span class="number">0.9599</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.stack((A,B),dim=<span class="number">2</span>) <span class="comment"># shape: [3, 3, 2]</span></span><br><span class="line">tensor([[[<span class="number">0.7021</span>, <span class="number">0.8316</span>],</span><br><span class="line">         [<span class="number">0.8243</span>, <span class="number">0.1665</span>],</span><br><span class="line">         [<span class="number">0.5045</span>, <span class="number">0.1453</span>]],</span><br><span class="line">        [[<span class="number">0.8906</span>, <span class="number">0.1797</span>],</span><br><span class="line">         [<span class="number">0.0391</span>, <span class="number">0.8958</span>],</span><br><span class="line">         [<span class="number">0.1455</span>, <span class="number">0.8249</span>]],</span><br><span class="line">        [[<span class="number">0.1793</span>, <span class="number">0.3707</span>],</span><br><span class="line">         [<span class="number">0.5437</span>, <span class="number">0.0720</span>],</span><br><span class="line">         [<span class="number">0.2779</span>, <span class="number">0.9599</span>]]])</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h1 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h1><p>Python中numpy数组的合并有很多方法，如<br>np.append()<br>np.concatenate()<br>np.stack()<br>np.hstack()<br>np.vstack()<br>np.dstack()<br>其中最泛用的是第一个和第二个。第一个可读性好，比较灵活，但是占内存大。第二个则没有内存占用大的问题。</p>
<h2 id="np-append"><a href="#np-append" class="headerlink" title="np.append()"></a>np.append()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=np.arange(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b=np.array([<span class="number">11</span>,<span class="number">22</span>,<span class="number">33</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.append(a,b)</span><br><span class="line">array([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>, <span class="number">11</span>, <span class="number">22</span>, <span class="number">33</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># 注意这个函数，numpy.append()函数每次都会重新分配整个数组，并把原来的数组复制到新数组中。</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a =np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b=np.array([[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.append(a,b)</span><br><span class="line">array([ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>])</span><br></pre></td></tr></table></figure>
<h2 id="np-concatenate"><a href="#np-concatenate" class="headerlink" title="np.concatenate()"></a>np.concatenate()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a =np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b=np.array([[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.concatenate((a,b),axis=<span class="number">0</span>)</span><br><span class="line">array([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">       [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">       [ <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">       [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.concatenate((a,b),axis=<span class="number">1</span>)  <span class="comment">#axis=1表示对应行的数组进行拼接</span></span><br><span class="line">array([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">       [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br></pre></td></tr></table></figure>
<h1 id="list"><a href="#list" class="headerlink" title="list"></a>list</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b=[<span class="number">10</span>,<span class="number">12</span>,<span class="number">15</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.extend(b)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>]</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://blog.csdn.net/qq_39516859/article/details/80666070">Python中numpy数组的拼接、合并</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数组堆叠</tag>
      </tags>
  </entry>
  <entry>
    <title>python标准化方法</title>
    <url>/archives/af7b2e2a.html</url>
    <content><![CDATA[<h1 id="第一类Normalization"><a href="#第一类Normalization" class="headerlink" title="第一类Normalization"></a>第一类Normalization</h1><h2 id="零一标准化"><a href="#零一标准化" class="headerlink" title="零一标准化"></a>零一标准化</h2><p>对数组列或行进行零一标准化，区间范围为0，1<br>对行进行零一标准化<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> minmax_scale</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = [[-<span class="number">1</span>, <span class="number">2</span>], [-<span class="number">0.5</span>, <span class="number">6</span>], [<span class="number">0</span>, <span class="number">10</span>], [<span class="number">1</span>, <span class="number">18</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>processed_data = minmax_scale(data,axis=<span class="number">1</span>)</span><br><span class="line">array([[<span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure><br>MinMaxScaler调用了minmax_scale，对列进行零一标准化<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = [[-<span class="number">1</span>, <span class="number">2</span>], [-<span class="number">0.5</span>, <span class="number">6</span>], [<span class="number">0</span>, <span class="number">10</span>], [<span class="number">1</span>, <span class="number">18</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler = MinMaxScaler()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler.fit(data)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>processed_data = scaler.transform(data)</span><br><span class="line">array([[<span class="number">0.</span>  , <span class="number">0.</span>  ],</span><br><span class="line">       [<span class="number">0.25</span>, <span class="number">0.25</span>],</span><br><span class="line">       [<span class="number">0.5</span> , <span class="number">0.5</span> ],</span><br><span class="line">       [<span class="number">1.</span>  , <span class="number">1.</span>  ]])</span><br></pre></td></tr></table></figure></p>
<h2 id="Z-score-normalization"><a href="#Z-score-normalization" class="headerlink" title="Z-score normalization"></a>Z-score normalization</h2><p>使用sklearn.preprocessing.scale()函数，可以直接将给定数据进行标准化。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = np.array([[ <span class="number">1.</span>, -<span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line"><span class="meta">... </span>              [ <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line"><span class="meta">... </span>              [ <span class="number">0.</span>,  <span class="number">1.</span>, -<span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_scaled = preprocessing.scale(X)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_scaled                                          </span><br><span class="line">array([[ <span class="number">0.</span>  ..., -<span class="number">1.22</span>...,  <span class="number">1.33</span>...],</span><br><span class="line">       [ <span class="number">1.22</span>...,  <span class="number">0.</span>  ..., -<span class="number">0.26</span>...],</span><br><span class="line">       [-<span class="number">1.22</span>...,  <span class="number">1.22</span>..., -<span class="number">1.06</span>...]])</span><br><span class="line">&gt;&gt;&gt;<span class="comment">#处理后数据的均值和方差</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_scaled.mean(axis=<span class="number">0</span>)</span><br><span class="line">array([ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_scaled.std(axis=<span class="number">0</span>)</span><br><span class="line">array([ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>])</span><br></pre></td></tr></table></figure></p>
<h1 id="第二类Normalization"><a href="#第二类Normalization" class="headerlink" title="第二类Normalization"></a>第二类Normalization</h1><p>第二种Normalization对于每个样本缩放到单位范数（每个样本的范数为1），主要有L1-normalization（L1范数）、L2-normalization（L2范数）等。<br>Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数（比如l1-norm,l2-norm）等于1。<br>p-范数的计算公式：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\|X\|_&#123;p&#125;&#x3D;\left(\left(\left|x_&#123;1&#125;\right|\right)^&#123;p&#125;+\left(\left|x_&#123;2&#125;\right|\right)^&#123;p&#125;+\ldots+\left(\left|x_&#123;n&#125;\right|\right)^&#123;p&#125;\right)^&#123;\frac&#123;1&#125;&#123;p&#125;&#125;</span><br></pre></td></tr></table></figure><br>该方法主要应用于文本分类和聚类中。例如，对于两个TF-IDF向量的l2-norm进行点积，就可以得到这两个向量的余弦相似性。</p>
<ul>
<li>可以使用preprocessing.normalize()函数对指定数据进行转换：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; X &#x3D; [[ 1., -1.,  2.],</span><br><span class="line">...      [ 2.,  0.,  0.],</span><br><span class="line">...      [ 0.,  1., -1.]]</span><br><span class="line">&gt;&gt;&gt; X_normalized &#x3D; preprocessing.normalize(X, norm&#x3D;&#39;l2&#39;)</span><br><span class="line">&gt;&gt;&gt; X_normalized                                      </span><br><span class="line">array([[ 0.40..., -0.40...,  0.81...],</span><br><span class="line">       [ 1.  ...,  0.  ...,  0.  ...],</span><br><span class="line">       [ 0.  ...,  0.70..., -0.70...]])</span><br><span class="line">&gt;&gt;&gt; X_normalized &#x3D; preprocessing.normalize(X, norm&#x3D;&#39;l1&#39;)</span><br><span class="line">array([[ 0.25, -0.25,  0.5 ],</span><br><span class="line">       [ 1.  ,  0.  ,  0.  ],</span><br><span class="line">       [ 0.  ,  0.5 , -0.5 ]])</span><br></pre></td></tr></table></figure>
<h2 id="一行或一列累加标准化"><a href="#一行或一列累加标准化" class="headerlink" title="一行或一列累加标准化"></a>一行或一列累加标准化</h2></li>
<li>使用preprocessing.normalize(X, norm=’l1’)<ul>
<li>可以避免全零行的影响</li>
<li>可以使用稀疏矩阵，必须CSR format </li>
</ul>
</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://blog.csdn.net/u011092188/article/details/78174804">Normalization(标准化)的原理和实现详解</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>标准化</tag>
      </tags>
  </entry>
  <entry>
    <title>python数组扩充</title>
    <url>/archives/13c6676c.html</url>
    <content><![CDATA[<h1 id="数组扩充"><a href="#数组扩充" class="headerlink" title="数组扩充"></a>数组扩充</h1><p>用repeat和tile扩充数组元素，例如</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.tile(data,(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=np.array([[<span class="number">10</span>,<span class="number">20</span>],[<span class="number">30</span>,<span class="number">40</span>]])  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.repeat([<span class="number">3</span>,<span class="number">2</span>],axis=<span class="number">0</span>)</span><br><span class="line">array([[<span class="number">10</span>, <span class="number">20</span>],  </span><br><span class="line">       [<span class="number">10</span>, <span class="number">20</span>],  </span><br><span class="line">       [<span class="number">10</span>, <span class="number">20</span>],  </span><br><span class="line">       [<span class="number">30</span>, <span class="number">40</span>],  </span><br><span class="line">       [<span class="number">30</span>, <span class="number">40</span>]]) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.repeat([<span class="number">3</span>,<span class="number">2</span>],axis=<span class="number">1</span>) </span><br><span class="line">array([[<span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">20</span>],</span><br><span class="line">       [<span class="number">30</span>, <span class="number">30</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">40</span>]])</span><br></pre></td></tr></table></figure>
<h1 id="torch数组扩充"><a href="#torch数组扩充" class="headerlink" title="torch数组扩充"></a>torch数组扩充</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = torch.Tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">3.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = data.repeat(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">3.</span>, <span class="number">2.</span>, <span class="number">3.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a.view(-<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">3.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = data.repeat(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">3.</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数组扩充</tag>
      </tags>
  </entry>
  <entry>
    <title>python添加进度条</title>
    <url>/archives/83a60a30.html</url>
    <content><![CDATA[<h2 id="方法1-tqdm"><a href="#方法1-tqdm" class="headerlink" title="方法1: tqdm"></a>方法1: tqdm</h2><ol>
<li>通过tqdm实现</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tqdm(list)方法可以传入任意一种list,比如数组</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">1000</span>)):</span><br><span class="line">	<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<ol>
<li>通过trange实现<br>trange(i) 是 tqdm(range(i)) 的简单写法</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> trange</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> trange(<span class="number">100</span>):</span><br><span class="line">	<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意，tqdm是任务开始时刷新进度条</p>
</blockquote>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://blog.csdn.net/zkp_987/article/details/81748098">tqdm介绍及常用方法</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>进度条</tag>
      </tags>
  </entry>
  <entry>
    <title>python相似度计算</title>
    <url>/archives/f8cf55af.html</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>相似度计算</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch loss function 总结</title>
    <url>/archives/8c962b5c.html</url>
    <content><![CDATA[<p>记录自己的学习过程，对深度学习目标函数自我总结。<br><a id="more"></a></p>
<h1 id="多标签分类（多目标分类）"><a href="#多标签分类（多目标分类）" class="headerlink" title="多标签分类（多目标分类）"></a>多标签分类（多目标分类）</h1><h2 id="BCELOSS"><a href="#BCELOSS" class="headerlink" title="BCELOSS"></a>BCELOSS</h2><p>等同于$torch.nn.functional.binary_cross_entropy$<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\ell(x, y)&#x3D;L&#x3D;\left\&#123;l_&#123;1&#125;, \ldots, l_&#123;N&#125;\right\&#125;^&#123;\top&#125;, \quad l_&#123;n&#125;&#x3D;-w_&#123;n&#125;\left[y_&#123;n&#125; \cdot \log x_&#123;n&#125;+\left(1-y_&#123;n&#125;\right) \cdot \log \left(1-x_&#123;n&#125;\right)\right]</span><br></pre></td></tr></table></figure><br>用于自编码器的重构误差，目标y在0到1之间。若$x_n$为0或1，则不稳定，因为log(0)无限</p>
<blockquote>
<p>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets yy should be numbers between 0 and 1.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Sigmoid()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = nn.BCELoss()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([ <span class="number">0.3216</span>, -<span class="number">1.3915</span>,  <span class="number">1.1682</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>target = torch.empty(<span class="number">3</span>).random_(<span class="number">2</span>)</span><br><span class="line">tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = loss(m(<span class="built_in">input</span>), target)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.backward()</span><br></pre></td></tr></table></figure>
<h2 id="BCEWithLogitsLoss"><a href="#BCEWithLogitsLoss" class="headerlink" title="BCEWithLogitsLoss"></a>BCEWithLogitsLoss</h2><blockquote>
<p>This loss combines a Sigmoid layer and the BCELoss in one single class.</p>
</blockquote>
<p>等同于$torch.nn.functional.binary_cross_entropy_with_logits$,有sigmoid层，网络层输出存在大于小于0的情况,<br>损失如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\ell(x, y)&#x3D;L&#x3D;\left\&#123;l_&#123;1&#125;, \ldots, l_&#123;N&#125;\right\&#125;^&#123;\top&#125;, \quad l_&#123;n&#125;&#x3D;-w_&#123;n&#125;\left[y_&#123;n&#125; \cdot \log \sigma\left(x_&#123;n&#125;\right)+\left(1-y_&#123;n&#125;\right) \cdot \log \left(1-\sigma\left(x_&#123;n&#125;\right)\right)\right]</span><br></pre></td></tr></table></figure><br>目标范围在0到1之间。可用于多标签分类，损失如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\ell_&#123;c&#125;(x, y)&#x3D;L_&#123;c&#125;&#x3D;\left\&#123;l_&#123;1, c&#125;, \ldots, l_&#123;N, c&#125;\right\&#125;^&#123;\top&#125;, \quad l_&#123;n, c&#125;&#x3D;-w_&#123;n, c&#125;\left[p_&#123;c&#125; y_&#123;n, c&#125; \cdot \log \sigma\left(x_&#123;n, c&#125;\right)+\left(1-y_&#123;n, c&#125;\right) \cdot \log (1-\sigma(x_&#123;n,c&#125;))]\right.</span><br></pre></td></tr></table></figure><br>其中，pos_weight提升正样例损失权重,维度与类个数相同，weight是对实例权重缩放，维度与nbatch相同<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>target = torch.ones([<span class="number">10</span>, <span class="number">64</span>], dtype=torch.float32)  <span class="comment"># 64 classes, batch size = 10</span></span><br><span class="line"><span class="number">10</span>*<span class="number">64</span>的全为<span class="number">1</span>的矩阵</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = torch.full([<span class="number">10</span>, <span class="number">64</span>], <span class="number">1.5</span>)  <span class="comment"># A prediction (logit)</span></span><br><span class="line"><span class="number">10</span>*<span class="number">64</span>的全为<span class="number">1.5</span>的矩阵</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pos_weight = torch.ones([<span class="number">64</span>])  <span class="comment"># All weights are equal to 1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>criterion(output, target)  <span class="comment"># -log(sigmoid(1.5))</span></span><br><span class="line">tensor(<span class="number">0.2014</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="MultiLabelMarginLoss"><a href="#MultiLabelMarginLoss" class="headerlink" title="MultiLabelMarginLoss"></a>MultiLabelMarginLoss</h2><p>多类多分类hinge loss.损失如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\operatorname&#123;loss&#125;(x, y)&#x3D;\sum_&#123;i j&#125; \frac&#123;\max (0,1-(x[y[j]]-x[i]))&#125;&#123;\mathrm&#123;x&#125; \cdot \operatorname&#123;size&#125;(0)&#125;</span><br></pre></td></tr></table></figure><br>其中，$i\neq y[j]$ for all i and j.我的理解是的有标签数据的分类概率要大于无标签的<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = nn.MultiLabelMarginLoss()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.FloatTensor([[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.8</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># for target y, only consider labels 3 and 0, not after label -1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.LongTensor([[<span class="number">3</span>, <span class="number">0</span>, -<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss(x, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))</span></span><br><span class="line">tensor(<span class="number">0.8500</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="MultiLabelSoftMarginLoss"><a href="#MultiLabelSoftMarginLoss" class="headerlink" title="MultiLabelSoftMarginLoss"></a>MultiLabelSoftMarginLoss</h2><p>MultiLabelSoftMarginLoss针对multi-label one-versus-all的情形,多个单分类。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\operatorname&#123;loss&#125;(x, y)&#x3D;-\frac&#123;1&#125;&#123;C&#125; * \sum_&#123;i&#125; y[i] * \log \left((1+\exp (-x[i]))^&#123;-1&#125;\right)+(1-y[i]) * \log \left(\frac&#123;\exp (-x[i])&#125;&#123;(1+\exp (-x[i]))&#125;\right)</span><br></pre></td></tr></table></figure></p>
<h1 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h1><h2 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h2><blockquote>
<p>It is useful when training a classification problem with C classes. If provided, the optional argument <strong>weight</strong> should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.<br>This criterion combines <strong>LogSoftmax</strong> and <strong>NLLLoss</strong> in <strong>one single class</strong>.</p>
</blockquote>
<p>x属于a的概率，网络的最后一层为原始的未标准化的分数<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\operatorname&#123;loss&#125;(x, \text &#123; class &#125;)&#x3D;-\log \left(\frac&#123;\exp (x[\text &#123; class &#125;])&#125;&#123;\sum_&#123;j&#125; \exp (x[j])&#125;\right)&#x3D;-x[\text &#123; class &#125;]+\log \left(\sum_&#123;j&#125; \exp (x[j])\right)</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>target = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line">tensor([<span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = loss(<span class="built_in">input</span>, target)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.backward()</span><br></pre></td></tr></table></figure></p>
<h2 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss"></a>NLLLoss</h2><p>NLLLoss的输入是一个对数概率向量和一个目标标签. 它不会为我们计算对数概率. 适合网络的最后一层是<strong>log_softmax</strong>. 损失函数 nn.CrossEntropyLoss() 与 NLLLoss() 相同, 唯一的不同是它为我们去做 softmax.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">m = nn.LogSoftmax()</span><br><span class="line">loss = nn.NLLLoss()</span><br><span class="line"><span class="comment"># input is of size nBatch x nClasses = 3 x 5</span></span><br><span class="line"><span class="built_in">input</span> = autograd.Variable(torch.randn(<span class="number">3</span>, <span class="number">5</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># each element in target has to have 0 &lt;= value &lt; nclasses</span></span><br><span class="line">target = autograd.Variable(torch.LongTensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>]))</span><br><span class="line">output = loss(m(<span class="built_in">input</span>), target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure></p>
<h1 id="分布一致性"><a href="#分布一致性" class="headerlink" title="分布一致性"></a>分布一致性</h1><h2 id="KLDivLoss"><a href="#KLDivLoss" class="headerlink" title="KLDivLoss"></a>KLDivLoss</h2><p>目标Tensor和输入Tensor大小一致。未约化的损失为<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l(x, y)&#x3D;L&#x3D;\left\&#123;l_&#123;1&#125;, \ldots, l_&#123;N&#125;\right\&#125;, \quad l_&#123;n&#125;&#x3D;y_&#123;n&#125; \cdot\left(\log y_&#123;n&#125;-x_&#123;n&#125;\right)</span><br></pre></td></tr></table></figure><br>mean和sum求均值以及求和。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.cnblogs.com/kk17/p/10246324.html">pytorch loss function 总结</a></li>
<li><a href="https://pytorch-cn.readthedocs.io/zh/latest/">PyTorch中文文档</a></li>
<li><a href="https://pytorch.org/docs/stable/index.html">Pytorch官方文档</a></li>
<li><a href="https://blog.csdn.net/shanglianlm/article/details/85019768">Pytorch学习之十九种损失函数</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch入门学习</title>
    <url>/archives/8c4d4d4a.html</url>
    <content><![CDATA[<h1 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h1><ul>
<li>从数据中生成<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>],[<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br></pre></td></tr></table></figure></li>
<li>从numpy数组中生成<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np_array = np.array(data)</span><br><span class="line">x_np = torch.from_numpy(np_array)</span><br></pre></td></tr></table></figure></li>
<li>从其他tensor中生成<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_ones = torch.ones_like(x_data) <span class="comment"># retains the properties of x_data</span></span><br><span class="line">x_rand = torch.rand_like(x_data, dtype=torch.<span class="built_in">float</span>) <span class="comment"># overrides the datatype of x_data</span></span><br></pre></td></tr></table></figure>
<h2 id="Tensor的属性"><a href="#Tensor的属性" class="headerlink" title="Tensor的属性"></a>Tensor的属性</h2>Tensor的属性包括shape，datatype和device<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">print(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;Datatype of tensor: <span class="subst">&#123;tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="tensor转numpy"><a href="#tensor转numpy" class="headerlink" title="tensor转numpy"></a>tensor转numpy</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = x.numpy()</span><br><span class="line">x_np = x.data.cpu().numpy() <span class="comment"># cuda转numpy</span></span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu perl运行踩坑汇总</title>
    <url>/archives/ee2c00b4.html</url>
    <content><![CDATA[<p>这些是我在运行一个pl脚本时遇到的一些错误，通过安装缺失的包解决了。</p>
<ol>
<li>Can’t locate DBI.pm in @INC (you may need to install the DBI module)</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libdbi-perl</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<ol>
<li></li>
</ol>
]]></content>
      <categories>
        <category>Perl</category>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>Perl</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu安装mysql和简单操作</title>
    <url>/archives/828b22fd.html</url>
    <content><![CDATA[<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install mysql-server          &#x2F;&#x2F;服务端</span><br><span class="line">sudo apt-get install mysql-client          &#x2F;&#x2F;客户端</span><br><span class="line">sudo apt-get install libmysqlclient-dev  &#x2F;&#x2F;程序编译时链接的库</span><br><span class="line">sudo apt-get install mysql-workbench  &#x2F;&#x2F;可视化工具</span><br></pre></td></tr></table></figure>
<p>安装过程中会提示设置密码什么的，注意设置了不要忘了，安装完成之后可以使用如下命令来检查是否安装成功：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo netstat -tap | grep mysql</span><br></pre></td></tr></table></figure><br><a id="more"></a></p>
<h1 id="登陆"><a href="#登陆" class="headerlink" title="登陆"></a>登陆</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -u root -p</span><br></pre></td></tr></table></figure>
<p>或者这样登陆</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#服务启动后端口查询</span><br><span class="line">sudo netstat -anp | grep mysql</span><br><span class="line"></span><br><span class="line">#连接数据库</span><br><span class="line">mysql -h 127.0.0.1 -P 3306 -uroot -p123456</span><br><span class="line">#-h为远程IP，-P为端口号，-u为用户名，-p为密码</span><br></pre></td></tr></table></figure>
<h2 id="可视化工具推荐"><a href="#可视化工具推荐" class="headerlink" title="可视化工具推荐"></a>可视化工具推荐</h2><p>Navicat Premiun，适合win和mac，可远程连接服务器数据库</p>
<h1 id="sql简单语法"><a href="#sql简单语法" class="headerlink" title="sql简单语法"></a>sql简单语法</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">show databases;  # 查看当前数据库</span><br><span class="line">create database library; # 创建数据库</span><br><span class="line">use library; # 选择数据库</span><br><span class="line">show tables; # 查看数据表</span><br><span class="line">exit; # 退出</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu系统深度学习环境搭建</title>
    <url>/archives/923d283b.html</url>
    <content><![CDATA[<p>首先，查看ubuntu版本可以用下面的指令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &#x2F;proc&#x2F;version</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h1 id="使用conda安装"><a href="#使用conda安装" class="headerlink" title="使用conda安装"></a>使用conda安装</h1><p>这是最简洁的安装方式</p>
<p>安装tensorflow-gpu<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda create -n tf-gpu tensorflow-gpu</span><br><span class="line">conda activate tf-gpu</span><br></pre></td></tr></table></figure><br>安装pytorch<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda install pytorch&#x3D;&#x3D;1.4.0 torchvision&#x3D;&#x3D;0.5.0 cudatoolkit&#x3D;10.1 -c pytorch</span><br><span class="line"># or</span><br><span class="line">conda install pytorch torchvision torchaudio cudatoolkit&#x3D;10.2 -c pytorch</span><br></pre></td></tr></table></figure><br>查看cuda版本</p>
<h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure></h2><p><strong>之后内容废弃</strong></p>
<h1 id="ubuntu18-04安装tensorflow"><a href="#ubuntu18-04安装tensorflow" class="headerlink" title="ubuntu18.04安装tensorflow"></a>ubuntu18.04安装tensorflow</h1><p>安装tensorflow，参考<a href="https://www.tensorflow.org/install/gpu">https://www.tensorflow.org/install/gpu</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Add NVIDIA package repositories</span><br><span class="line">wget https:&#x2F;&#x2F;developer.download.nvidia.com&#x2F;compute&#x2F;cuda&#x2F;repos&#x2F;ubuntu1804&#x2F;x86_64&#x2F;cuda-ubuntu1804.pin</span><br><span class="line">sudo mv cuda-ubuntu1804.pin &#x2F;etc&#x2F;apt&#x2F;preferences.d&#x2F;cuda-repository-pin-600</span><br><span class="line">sudo apt-key adv --fetch-keys https:&#x2F;&#x2F;developer.download.nvidia.com&#x2F;compute&#x2F;cuda&#x2F;repos&#x2F;ubuntu1804&#x2F;x86_64&#x2F;7fa2af80.pub</span><br><span class="line">sudo add-apt-repository &quot;deb https:&#x2F;&#x2F;developer.download.nvidia.com&#x2F;compute&#x2F;cuda&#x2F;repos&#x2F;ubuntu1804&#x2F;x86_64&#x2F; &#x2F;&quot;</span><br><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line">wget http:&#x2F;&#x2F;developer.download.nvidia.com&#x2F;compute&#x2F;machine-learning&#x2F;repos&#x2F;ubuntu1804&#x2F;x86_64&#x2F;nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb</span><br><span class="line"></span><br><span class="line">sudo apt install .&#x2F;nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line"># Install NVIDIA driver</span><br><span class="line">sudo apt-get install --no-install-recommends nvidia-driver-450</span><br><span class="line"># Reboot. Check that GPUs are visible using the command: nvidia-smi</span><br><span class="line"></span><br><span class="line">wget https:&#x2F;&#x2F;developer.download.nvidia.com&#x2F;compute&#x2F;machine-learning&#x2F;repos&#x2F;ubuntu1804&#x2F;x86_64&#x2F;libnvinfer7_7.1.3-1+cuda11.0_amd64.deb</span><br><span class="line">sudo apt install .&#x2F;libnvinfer7_7.1.3-1+cuda11.0_amd64.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line"># Install development and runtime libraries (~4GB)</span><br><span class="line">sudo apt-get install --no-install-recommends \</span><br><span class="line">    cuda-11-0 \</span><br><span class="line">    libcudnn8&#x3D;8.0.4.30-1+cuda11.0  \</span><br><span class="line">    libcudnn8-dev&#x3D;8.0.4.30-1+cuda11.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Install TensorRT. Requires that libcudnn8 is installed above.</span><br><span class="line">sudo apt-get install -y --no-install-recommends libnvinfer7&#x3D;7.1.3-1+cuda11.0 \</span><br><span class="line">    libnvinfer-dev&#x3D;7.1.3-1+cuda11.0 \</span><br><span class="line">    libnvinfer-plugin7&#x3D;7.1.3-1+cuda11.0</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="匹配关系"><a href="#匹配关系" class="headerlink" title="匹配关系"></a>匹配关系</h2><p>tensorflow匹配关系见网址：<a href="https://www.tensorflow.org/install/source_windows">https://www.tensorflow.org/install/source_windows</a></p>
<h2 id="tensroflow-GPU测试代码"><a href="#tensroflow-GPU测试代码" class="headerlink" title="tensroflow GPU测试代码"></a>tensroflow GPU测试代码</h2><figure class="highlight diff"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"><span class="deletion">- print(tf.test.is_gpu_available()) # 之前的版本，待废弃</span></span><br><span class="line"><span class="addition">+ tf.config.list_physical_devices(&#x27;GPU&#x27;)</span></span><br></pre></td></tr></table></figure>
<h1 id="ubuntu18-04安装pytorch"><a href="#ubuntu18-04安装pytorch" class="headerlink" title="ubuntu18.04安装pytorch"></a>ubuntu18.04安装pytorch</h1><p>在安装好nvidia驱动的基础上，可以直接安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision -c pytorch</span><br></pre></td></tr></table></figure>
<h2 id="匹配关系-1"><a href="#匹配关系-1" class="headerlink" title="匹配关系"></a>匹配关系</h2><p>这里是pytorch和cudatoolkit版本对应关系：<a href="https://pytorch.org/get-started/previous-versions/">https://pytorch.org/get-started/previous-versions/</a></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>CUDA版本</th>
<th>可用PyTorch版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>7.5</td>
<td>0.4.1 ，0.3.0， 0.2.0，0.1.12-0.1.6</td>
</tr>
<tr>
<td>8.0</td>
<td>1.1.0，1.0.0 ，0.4.1</td>
</tr>
<tr>
<td>9.0</td>
<td>1.1.0，1.0.1, 1.0.0，0.4.1</td>
</tr>
<tr>
<td>9.2</td>
<td>1.6.0，1.5.0，1.4.0，1.2.0，0.4.1</td>
</tr>
<tr>
<td>10.0</td>
<td>1.2.0，1.1.0，1.0.1 ,1.0.0</td>
</tr>
<tr>
<td>10.1</td>
<td>1.6.0，1.5.0， 1.4.0，1.3.0</td>
</tr>
<tr>
<td>10.2</td>
<td>1.6.0，1.5.0</td>
</tr>
<tr>
<td>11.0</td>
<td>1.7.0</td>
</tr>
</tbody>
</table>
</div>
<h2 id="pytorch-GPU测试代码"><a href="#pytorch-GPU测试代码" class="headerlink" title="pytorch GPU测试代码"></a>pytorch GPU测试代码</h2><h2 id="-1"><a href="#-1" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">print(torch.cuda.is_available())</span><br></pre></td></tr></table></figure></h2><h1 id="Nvidia驱动、cuda和cuDNN安装"><a href="#Nvidia驱动、cuda和cuDNN安装" class="headerlink" title="Nvidia驱动、cuda和cuDNN安装"></a>Nvidia驱动、cuda和cuDNN安装</h1><h2 id="NVIDIA-驱动安装"><a href="#NVIDIA-驱动安装" class="headerlink" title="NVIDIA 驱动安装"></a>NVIDIA 驱动安装</h2><h2 id="CUDA安装"><a href="#CUDA安装" class="headerlink" title="CUDA安装"></a>CUDA安装</h2><p>首先贴一张CUDA和驱动版本的对应表，具体可见<a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#abstract">https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#abstract</a>。<br><img src="http://ww1.sinaimg.cn/large/006ltHaXgy1gns49hgbu1j315m0rqaf4.jpg" alt="QQ20210218-233600@2x.png"></p>
<p>去官网上 下载和驱动对应的cuda文件<a href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a>，需要注册一个账号。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.cnblogs.com/carle-09/p/11252814.html">Ubuntu系统—-安NVIDIA 驱动后  CUDA+cuDNN 安装</a></li>
<li><a href="https://www.cnblogs.com/Wanggcong/p/12625540.html">pytorch版本，cuda版本，系统cuda版本查询和对应关系</a></li>
<li><a href="https://www.cnblogs.com/sunshe35/articles/12808780.html">彻底搞定tensorflow-GPU环境(史上最全)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/52498335">conda安装tensorflow和pytorch</a></li>
</ol>
]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>cuda</tag>
        <tag>cuDNN</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu终端配置代理</title>
    <url>/archives/6a15ee01.html</url>
    <content><![CDATA[<h1 id="终端代理配置"><a href="#终端代理配置" class="headerlink" title="终端代理配置"></a>终端代理配置</h1><h2 id="临时"><a href="#临时" class="headerlink" title="临时"></a>临时</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export https_proxy&#x3D;https:&#x2F;&#x2F;用户名:密码@代理地址:代理端口</span><br><span class="line">export http_proxy&#x3D;http:&#x2F;&#x2F;用户名:密码@代理地址:代理端口</span><br></pre></td></tr></table></figure>
<h2 id="永久"><a href="#永久" class="headerlink" title="永久"></a>永久</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># gedit ~&#x2F;.bashrc 在.bashrc文件末尾添加如下内容，或者在&#x2F;etc&#x2F;environment中</span><br><span class="line">export http_proxy&#x3D;http:&#x2F;&#x2F;用户名:密码@地址:端口&#x2F;</span><br><span class="line">export https_proxy&#x3D;http:&#x2F;&#x2F;用户名:密码@地址:端口&#x2F;</span><br><span class="line">export ftp_proxy&#x3D;http:&#x2F;&#x2F;用户名:密码@地址:端口&#x2F;</span><br><span class="line"># sudo gedit &#x2F;etc&#x2F;apt&#x2F;apt.conf 在apt.conf文件中加入下面这行</span><br><span class="line">Acquire::http::proxy &quot;http:&#x2F;&#x2F;用户名:密码@地址:端口&quot;;</span><br><span class="line">Acquire::ftp::proxy &quot;http:&#x2F;&#x2F;用户名:密码@地址:端口&quot;;</span><br><span class="line">Acquire::https::proxy &quot;http:&#x2F;&#x2F;用户名:密码@地址:端口&quot;;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h1 id="配置V2ray-socks5"><a href="#配置V2ray-socks5" class="headerlink" title="配置V2ray socks5"></a>配置V2ray socks5</h1><h2 id="在ubuntu配置v2ray客户端"><a href="#在ubuntu配置v2ray客户端" class="headerlink" title="在ubuntu配置v2ray客户端"></a>在ubuntu配置v2ray客户端</h2><p>首先下载脚本:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;github.com&#x2F;v2fly&#x2F;fhs-install-v2ray&#x2F;blob&#x2F;master&#x2F;install-dat-release.sh</span><br></pre></td></tr></table></figure>
<p>然后执行脚本安装 V2Ray:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo bash install-dat-release.sh</span><br></pre></td></tr></table></figure>
<p>复制配置文件config.json到<strong>/usr/local/etc/v2ray/config.json</strong>,可以直接从windows或mac的v2rayN客户端夹中直接复制过来.</p>
<p>启动v2ray服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl restart v2ray</span><br></pre></td></tr></table></figure>
<p>socks5代理已经启动, 默认端口是1080, (socks5://127.0.0.1:1080).</p>
<h2 id="配置http代理-privoxy"><a href="#配置http代理-privoxy" class="headerlink" title="配置http代理 privoxy"></a>配置http代理 privoxy</h2><p>有些命令行工具只能使用http代理, 不能使用socks5代理, 因此需要用privoxy把socks5代理转换为http代理.</p>
<p>安装privoxy</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install -y privoxy</span><br></pre></td></tr></table></figure>
<p>修改配置文件/etc/privoxy/config</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">listen-address  :10809</span><br><span class="line">forward-socks5    &#x2F;    127.0.0.1:10808  .</span><br></pre></td></tr></table></figure>
<p>启动privoxy服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl restart privoxy</span><br></pre></td></tr></table></figure>
<p>http代理已经启动, 默认端口是10809, (<a href="http://127.0.0.1:10809">http://127.0.0.1:10809</a>).</p>
<h2 id="配置proxychains"><a href="#配置proxychains" class="headerlink" title="配置proxychains"></a>配置proxychains</h2><p>有些linux命令行工具没有配置代理的方法, 可以用proxychains强制应用使用代理网络.</p>
<p>安装proxychains</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install -y proxychains</span><br></pre></td></tr></table></figure>
<p>修改配置文件/etc/proxychains.conf最后一行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">socks5  127.0.0.1 10808</span><br></pre></td></tr></table></figure>
<p>使用proxychains方法, 在命令前加上proxychains, 如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">proxychains apt update</span><br></pre></td></tr></table></figure>
<h2 id="python-pip使用http代理加速"><a href="#python-pip使用http代理加速" class="headerlink" title="python pip使用http代理加速"></a>python pip使用http代理加速</h2><p>方法1:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip3 install -r requirement.txt --proxy http:&#x2F;&#x2F;127.0.0.1:10809</span><br></pre></td></tr></table></figure>
<p>方法2:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">proxychains pip3 install -r requirement.txt</span><br></pre></td></tr></table></figure>
<p>git使用http代理加速</p>
<p>方法1:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config --global http.proxy http:&#x2F;&#x2F;127.0.0.1:10809</span><br><span class="line">git config --global https.proxy http:&#x2F;&#x2F;127.0.0.1:10809</span><br></pre></td></tr></table></figure>
<p>取消设置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config --global --unset http.proxy</span><br></pre></td></tr></table></figure>
<p>方法2:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">proxychains git clone https:&#x2F;&#x2F;github.com&#x2F;opencv&#x2F;opencv.git</span><br></pre></td></tr></table></figure>
<p>docker使用http代理加速</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d</span><br><span class="line">&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;http-proxy.conf</span><br><span class="line">------</span><br><span class="line">[Service]</span><br><span class="line">Environment&#x3D;&quot;HTTP_PROXY&#x3D;http:&#x2F;&#x2F;127.0.0.1:10809&#x2F;&quot;</span><br><span class="line">------</span><br><span class="line"># systemctl daemon-reload</span><br><span class="line"># systemctl restart docker</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://blog.51cto.com/laok8/2609755">在Ubuntu Linux中命令行工具使用代理v2ray privoxy proxychains学习</a></li>
</ol>
]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title>vim个人配置记录</title>
    <url>/archives/fff3ae8c.html</url>
    <content><![CDATA[<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="spacevim安装"><a href="#spacevim安装" class="headerlink" title="spacevim安装"></a>spacevim安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sLf https:&#x2F;&#x2F;spacevim.org&#x2F;cn&#x2F;install.sh | bash</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="配置修改"><a href="#配置修改" class="headerlink" title="配置修改"></a>配置修改</h2><p>修改文件.SpaceVim.dinit.toml<br><figure class="highlight diff"><table><tr><td class="code"><pre><span class="line">[[layers]]</span><br><span class="line">  name = &quot;lang#markdown&quot;</span><br></pre></td></tr></table></figure></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://spacevim.org/cn/">SpaceVim</a></li>
</ol>
]]></content>
      <categories>
        <category>软件使用</category>
      </categories>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title>【天赋树】一个期权交易员的自我修养 - 转载</title>
    <url>/archives/bf7d175a.html</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="562fef4d32461ebd73b3a7d535a4c165b7862c4afada2fa94b226b8c5c21f055">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>交易</category>
        <category>期权</category>
      </categories>
      <tags>
        <tag>天赋树</tag>
      </tags>
  </entry>
  <entry>
    <title>使用NLTK进行文本处理</title>
    <url>/archives/42874102.html</url>
    <content><![CDATA[<h1 id="NLTK"><a href="#NLTK" class="headerlink" title="NLTK"></a>NLTK</h1><p>NLTK是Python很强大的第三方库，可以很方便的完成很多自然语言处理（NLP）的任务，包括分词、词性标注、命名实体识别（NER）及句法分析。</p>
<h2 id="NLTK进行分词"><a href="#NLTK进行分词" class="headerlink" title="NLTK进行分词"></a>NLTK进行分词</h2><p>使用的函数：</p>
<p>nltk.sent_tokenize(text) #对文本按照句子进行分割</p>
<p>nltk.word_tokenize(sent) #对句子进行分词<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> nltk</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = <span class="string">&quot;cslzhl.github.io is a very good blog. We can learn a lot from it.&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># 将文本拆分成句子列表，根据. ?等符号划分，逗号无效</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sen = nltk.sent_tokenize(text) </span><br><span class="line">[<span class="string">&#x27;cslzhl.github.io is a very good blog.&#x27;</span>, <span class="string">&#x27;We can learn a lot from it.&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>words = [nltk.word_tokenize(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> sen]</span><br><span class="line">[[<span class="string">&#x27;cslzhl.github.io&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;very&#x27;</span>, <span class="string">&#x27;good&#x27;</span>, <span class="string">&#x27;blog&#x27;</span>, <span class="string">&#x27;.&#x27;</span>], [<span class="string">&#x27;We&#x27;</span>, <span class="string">&#x27;can&#x27;</span>, <span class="string">&#x27;learn&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;lot&#x27;</span>, <span class="string">&#x27;from&#x27;</span>, <span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]]</span><br></pre></td></tr></table></figure><br>使用<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> WordPunctTokenizer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="string">&quot;Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks.&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>WordPunctTokenizer().tokenize(s) <span class="comment"># 字母和非字母分开</span></span><br><span class="line">[<span class="string">&#x27;Good&#x27;</span>, <span class="string">&#x27;muffins&#x27;</span>, <span class="string">&#x27;cost&#x27;</span>, <span class="string">&#x27;$&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;88&#x27;</span>, <span class="string">&#x27;in&#x27;</span>, <span class="string">&#x27;New&#x27;</span>, <span class="string">&#x27;York&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;Please&#x27;</span>, <span class="string">&#x27;buy&#x27;</span>, <span class="string">&#x27;me&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;them&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;Thanks&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>nltk.word_tokenize(s)</span><br><span class="line">[<span class="string">&#x27;Good&#x27;</span>, <span class="string">&#x27;muffins&#x27;</span>, <span class="string">&#x27;cost&#x27;</span>, <span class="string">&#x27;$&#x27;</span>, <span class="string">&#x27;3.88&#x27;</span>, <span class="string">&#x27;in&#x27;</span>, <span class="string">&#x27;New&#x27;</span>, <span class="string">&#x27;York&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;Please&#x27;</span>, <span class="string">&#x27;buy&#x27;</span>, <span class="string">&#x27;me&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;them&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;Thanks&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br></pre></td></tr></table></figure></p>
<h2 id="NLTK进行词性标注"><a href="#NLTK进行词性标注" class="headerlink" title="NLTK进行词性标注"></a>NLTK进行词性标注</h2><p>用到的函数：<br>nltk.pos_tag(tokens)#tokens是句子分词后的结果，同样是句子级的标注<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tags = [nltk.pos_tag(tokens) <span class="keyword">for</span> tokens <span class="keyword">in</span> words] </span><br><span class="line">[[(<span class="string">&#x27;cslzhl.github.io&#x27;</span>, <span class="string">&#x27;NN&#x27;</span>), (<span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;VBZ&#x27;</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;DT&#x27;</span>), (<span class="string">&#x27;very&#x27;</span>, <span class="string">&#x27;RB&#x27;</span>), (<span class="string">&#x27;good&#x27;</span>, <span class="string">&#x27;JJ&#x27;</span>), (<span class="string">&#x27;blog&#x27;</span>, <span class="string">&#x27;NN&#x27;</span>), (<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;.&#x27;</span>)], [(<span class="string">&#x27;We&#x27;</span>, <span class="string">&#x27;PRP&#x27;</span>), (<span class="string">&#x27;can&#x27;</span>, <span class="string">&#x27;MD&#x27;</span>), (<span class="string">&#x27;learn&#x27;</span>, <span class="string">&#x27;VB&#x27;</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;DT&#x27;</span>), (<span class="string">&#x27;lot&#x27;</span>, <span class="string">&#x27;NN&#x27;</span>), (<span class="string">&#x27;from&#x27;</span>, <span class="string">&#x27;IN&#x27;</span>), (<span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;PRP&#x27;</span>), (<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;.&#x27;</span>)]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># NN 名词，VB动词（原形），VBZ，动词（第三人称单数），DT限定词等等</span></span><br></pre></td></tr></table></figure></p>
<h2 id="NLTK词干提取（stemming）"><a href="#NLTK词干提取（stemming）" class="headerlink" title="NLTK词干提取（stemming）"></a>NLTK词干提取（stemming）</h2><p>Stemming 是抽取词的词干或词根形式（不一定能够表达完整语义）。NLTK中提供了三种最常用的词干提取器接口，即 Porter stemmer, Lancaster Stemmer 和 Snowball Stemmer。</p>
<ul>
<li>Porter Stemmer基于Porter词干提取算法，来看例子： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> nltk.stem.porter <span class="keyword">import</span> PorterStemmer  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>porter_stemmer = PorterStemmer()  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>porter_stemmer.stem(‘maximum’)  </span><br><span class="line">u’maximum’  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>porter_stemmer.stem(‘presumably’)  </span><br><span class="line">u’presum’  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>porter_stemmer.stem(‘multiply’)  </span><br><span class="line">u’multipli’  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>porter_stemmer.stem(‘provision’)  </span><br><span class="line">u’provis’  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>porter_stemmer.stem(‘owed’)  </span><br><span class="line">u’owe’  </span><br></pre></td></tr></table></figure></li>
<li>ancaster Stemmer 基于Lancaster 词干提取算法，来看例子<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> nltk.stem.lancaster <span class="keyword">import</span> LancasterStemmer  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lancaster_stemmer = LancasterStemmer()  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lancaster_stemmer.stem(‘maximum’)  </span><br><span class="line">‘maxim’  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lancaster_stemmer.stem(‘presumably’)  </span><br><span class="line">‘presum’  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lancaster_stemmer.stem(‘presumably’)  </span><br><span class="line">‘presum’  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lancaster_stemmer.stem(‘multiply’)  </span><br><span class="line">‘multiply’  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lancaster_stemmer.stem(‘provision’)  </span><br><span class="line">u’provid’  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lancaster_stemmer.stem(‘owed’)  </span><br><span class="line">‘ow’  </span><br></pre></td></tr></table></figure></li>
<li>Snowball Stemmer基于Snowball 词干提取算法，来看例子<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> SnowballStemmer  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>snowball_stemmer = SnowballStemmer(“english”)  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>snowball_stemmer.stem(‘maximum’)  </span><br><span class="line">u’maximum’  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>snowball_stemmer.stem(‘presumably’)  </span><br><span class="line">u’presum’  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>snowball_stemmer.stem(‘multiply’)  </span><br><span class="line">u’multipli’  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>snowball_stemmer.stem(‘provision’)  </span><br><span class="line">u’provis’  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>snowball_stemmer.stem(‘owed’)  </span><br><span class="line">u’owe’  </span><br></pre></td></tr></table></figure>
<blockquote>
<p>词形还原（lemmatization）<br>Lemmatisation是把一个任何形式的语言词汇还原为一般形式（能表达完整语义）。相对而言，词干提取是简单的轻量级的词形归并方式，最后获得的结果为词干，并不一定具有实际意义。词形还原处理相对复杂，获得结果为词的原形，能够承载一定意义，与词干提取相比，更具有研究和应用价值。</p>
</blockquote>
</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://blog.csdn.net/a18852867035/article/details/54134281/">python自然语言处理（一）NLTK初步使用</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>文本处理</tag>
        <tag>NLTK</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu查看文件编码类型</title>
    <url>/archives/e51b08cc.html</url>
    <content><![CDATA[<h2 id="方法1-file-i-filename"><a href="#方法1-file-i-filename" class="headerlink" title="方法1: file -i filename"></a>方法1: file -i filename</h2><a id="more"></a>]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>使用hexo搭建个人博客</title>
    <url>/archives/ba7bb759.html</url>
    <content><![CDATA[<p>前言：本博客搭建时使用的nodejs版本为12.21.1，hexo版本为5.3.0，next版本为7.4.0，初次搭建推荐做一些文件改动时就测试一下(我到最后才发现hexo d居然出问题，艹)<br><a id="more"></a></p>
<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><p>在开始一切之前，你必须已经：</p>
<ul>
<li>有一个github或gitee账号，没有的话去注册一个；</li>
<li>安装了node.js、npm，并了解相关基础知识</li>
</ul>
<p>本文所使用的环境：</p>
<ul>
<li>node.js@12.20.1</li>
<li>hexo@5.3.0</li>
<li>next@7.4.0</li>
</ul>
<p>npm推荐使用国内源，速度方便快捷<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 设置成淘宝源</span><br><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npm.taobao.org</span><br><span class="line"># 查看结果</span><br><span class="line">npm config get registry</span><br></pre></td></tr></table></figure></p>
<h1 id="使用hexo写博客"><a href="#使用hexo写博客" class="headerlink" title="使用hexo写博客"></a>使用hexo写博客</h1><h2 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-cli -g</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html">使用hexo+github搭建免费个人博客详细教程</a></li>
<li><a href="https://github.com/hexojs/hexo">https://github.com/hexojs/hexo</a></li>
<li><a href="https://blog.csdn.net/qq_39207948/article/details/79449633">npm配置国内镜像资源+淘宝镜像</a></li>
</ol>
]]></content>
      <categories>
        <category>建站教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github page</tag>
      </tags>
  </entry>
  <entry>
    <title>使用spacy进行文本处理</title>
    <url>/archives/e58dc34c.html</url>
    <content><![CDATA[<p>工业级Python自然语言处理软件包Spacy。Spacy 是由 cython 编写。因此它是一个非常快的库。 spaCy 提供简洁的接口用来访问其方法和属性（ governed by trained machine (and deep) learning models）<br><a id="more"></a></p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>安装Spacy<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install spacy</span><br></pre></td></tr></table></figure><br>下载数据和模型<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m spacy download en</span><br></pre></td></tr></table></figure><br>补充：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用conda安装，参考https://anaconda.org/conda-forge/spacy-model-en_vectors_web_lg</span></span><br><span class="line">conda install -c conda-forge spacy-model-en_vectors_web_lg</span><br><span class="line">conda install -c conda-forge/label/cf202003 spacy-model-en_vectors_web_lg</span><br></pre></td></tr></table></figure></p>
<h1 id="Spacy-流水线-和-属性"><a href="#Spacy-流水线-和-属性" class="headerlink" title="Spacy 流水线 和 属性"></a>Spacy 流水线 和 属性</h1><p>要想使用 Spacy 和 访问其不同的 properties， 需要先创建 pipelines。 通过加载 模型 来创建一个 pipeline。 Spacy 提供了许多不同的 模型 , 模型中包含了 语言的信息- 词汇表，预训练的词向量，语法 和 实体。</p>
<p>下面将加载默认的模型- english-core-web<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> spacy </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>nlp = spacy.load(“en”)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = <span class="string">&quot;The sequel, Yes, Prime Minister, ran from 1986 to 1988. In total there were 38 episodes, of which all but one lasted half an hour. Almost all episodes ended with a variation of the title of the series spoken as the answer to a question posed by the same character, Jim Hacker. Several episodes were adapted for BBC Radio, and a stage play was produced in 2010, the latter leading to a new television series on UKTV Gold in 2013.&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doc = nlp(text)</span><br><span class="line"><span class="string">&quot;The&quot;</span></span><br><span class="line"><span class="string">&quot;sequel&quot;</span></span><br><span class="line"><span class="string">&quot;,&quot;</span></span><br><span class="line"><span class="string">&quot;Yes&quot;</span>...</span><br></pre></td></tr></table></figure><br>document 也有一些 成员属性。可以通过 $dir(doc)$ 查看</p>
<p>下面，我们只对前10个词例（token），输出以下内容：<br>文本、索引值（即在原文中的定位）、词元(lemma)、是否为标点符号、是否为空格、词性、标记<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc[:<span class="number">10</span>]:</span><br><span class="line">    print(<span class="string">&quot;&#123;0&#125;\t&#123;1&#125;\t&#123;2&#125;\t&#123;3&#125;\t&#123;4&#125;\t&#123;5&#125;\t&#123;6&#125;\t&#123;7&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">        token.text,</span><br><span class="line">        token.idx,</span><br><span class="line">        token.lemma_,</span><br><span class="line">        token.is_punct,</span><br><span class="line">        token.is_space,</span><br><span class="line">        token.shape_,</span><br><span class="line">        token.pos_,</span><br><span class="line">        token.tag_</span><br><span class="line">    ))</span><br></pre></td></tr></table></figure><br>结果为<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The 0   the False   False   Xxx DET DT</span><br><span class="line">sequel  4   sequel  False   False   xxxx    NOUN    NN</span><br><span class="line">,   10  ,   True    False   ,   PUNCT   ,</span><br><span class="line">Yes 12  yes False   False   Xxx INTJ    UH</span><br><span class="line">,   15  ,   True    False   ,   PUNCT   ,</span><br><span class="line">Prime   17  prime   False   False   Xxxxx   PROPN   NNP</span><br><span class="line">Minister    23  minister    False   False   Xxxxx   PROPN   NNP</span><br><span class="line">,   31  ,   True    False   ,   PUNCT   ,</span><br><span class="line">ran 33  run False   False   xxx VERB    VBD</span><br><span class="line">from    37  from    False   False   xxxx    ADP IN</span><br></pre></td></tr></table></figure></p>
<h2 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>all_tags = &#123;w.pos: w.pos_ <span class="keyword">for</span> w <span class="keyword">in</span> doc&#125;</span><br><span class="line">&#123;<span class="number">90</span>: <span class="string">&#x27;DET&#x27;</span>,<span class="number">92</span>: <span class="string">&#x27;NOUN&#x27;</span>,<span class="number">97</span>: <span class="string">&#x27;PUNCT&#x27;</span>,<span class="number">91</span>: <span class="string">&#x27;INTJ&#x27;</span>,<span class="number">96</span>: <span class="string">&#x27;PROPN&#x27;</span>,<span class="number">100</span>: <span class="string">&#x27;VERB&#x27;</span>,<span class="number">85</span>: <span class="string">&#x27;ADP&#x27;</span>,<span class="number">93</span>: <span class="string">&#x27;NUM&#x27;</span>,<span class="number">95</span>: <span class="string">&#x27;PRON&#x27;</span>,<span class="number">87</span>: <span class="string">&#x27;AUX&#x27;</span>,<span class="number">98</span>: <span class="string">&#x27;SCONJ&#x27;</span>,<span class="number">86</span>: <span class="string">&#x27;ADV&#x27;</span>,<span class="number">84</span>: <span class="string">&#x27;ADJ&#x27;</span>,<span class="number">89</span>: <span class="string">&#x27;CCONJ&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<h2 id="实体检测"><a href="#实体检测" class="headerlink" title="实体检测"></a>实体检测</h2><p>Spacy 包含了一个快速的 实体识别模型，它可以识别出文档中的 实体短语。有多种类型的实体，例如 - 人物，地点，组织，日期，数字。可以通过 document 的 ents 属性来访问这些实体。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> ent <span class="keyword">in</span> doc.ents:</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    print(ent.text, ent.label_)</span><br></pre></td></tr></table></figure></p>
<h2 id="语句划分"><a href="#语句划分" class="headerlink" title="语句划分"></a>语句划分</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> sent <span class="keyword">in</span> doc.sents:</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    print(sent)</span><br></pre></td></tr></table></figure>
<h2 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h2><p>使用词嵌入模型，我们需要Spacy读取一个新的文件。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>nlp = spacy.load(<span class="string">&#x27;en_core_web_lg&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(nlp.vocab[<span class="string">&#x27;minister&#x27;</span>].vector) <span class="comment"># 读取单词对应的词向量</span></span><br></pre></td></tr></table></figure><br>词语义的近似度计算<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>dog = nlp.vocab[<span class="string">&quot;dog&quot;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cat = nlp.vocab[<span class="string">&quot;cat&quot;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dog.similarity(cat)</span><br><span class="line"><span class="number">0.80168545</span></span><br></pre></td></tr></table></figure><br>计算词典中可能不存在的向量，因此Spacy自带的similarity()函数，就显得不够用了。我们从scipy中，找到相似度计算需要用到的余弦函数。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> scipy.spatial.distance <span class="keyword">import</span> cosine</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">1</span> - cosine(dog.vector, cat.vector) <span class="comment"># 与上述结果相同</span></span><br><span class="line"><span class="number">0.8016855120658875</span></span><br></pre></td></tr></table></figure></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://blog.csdn.net/u012436149/article/details/79321112">使用 spacy 进行自然语言处理（一）</a></li>
<li><a href="https://www.jianshu.com/p/1ede048ef7e6">如何用Python处理自然语言？（Spacy与Word Embedding）</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>文本处理</tag>
        <tag>spacy</tag>
      </tags>
  </entry>
  <entry>
    <title>使用gensim进行文本处理</title>
    <url>/archives/ff4fc7ce.html</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>文本处理</tag>
        <tag>genssim</tag>
      </tags>
  </entry>
  <entry>
    <title>常用工具推荐</title>
    <url>/archives/ab83100f.html</url>
    <content><![CDATA[<p>仅记录我使用的一些比较方便的工具，留作备份<br><a id="more"></a></p>
<h1 id="win10"><a href="#win10" class="headerlink" title="win10"></a>win10</h1><ol>
<li>mobaxterm<br>本地电脑目录：/drives/下有所有盘符</li>
<li>vscode<br>markdown和latex等插件齐全。<br>markdown：Markdown All in One和Markdown Preview Enhanced</li>
<li>Mathpix Snip<br>latex公式识别神器</li>
<li>navicat<br>数据库可视化</li>
</ol>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>软件</tag>
      </tags>
  </entry>
  <entry>
    <title>学习排序算法</title>
    <url>/archives/4f2a5e3e.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>永续合约套利 - 转载</title>
    <url>/archives/351b7a85.html</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="24bead56daa2cb480630978723dbdf092704e85db2a6d736b699c9dfd935374b">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>交易</category>
      </categories>
      <tags>
        <tag>套利</tag>
      </tags>
  </entry>
  <entry>
    <title>统计检验</title>
    <url>/archives/d831d399.html</url>
    <content><![CDATA[<h1 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h1><h2 id="Wilcoxon-秩和检验"><a href="#Wilcoxon-秩和检验" class="headerlink" title="Wilcoxon 秩和检验"></a>Wilcoxon 秩和检验</h2>]]></content>
  </entry>
  <entry>
    <title>缺陷预测-软件度量</title>
    <url>/archives/5a3c8d60.html</url>
    <content><![CDATA[<p>参考部分百科<sup><a href="#fn_1" id="reffn_1">1</a></sup><sup><a href="#fn_2" id="reffn_2">2</a></sup>和英文文献<br><a id="more"></a></p>
<h1 id="EccentricityHage"><a href="#EccentricityHage" class="headerlink" title="EccentricityHage"></a>Eccentricity<sup><a href="#fn_Hage" id="reffn_Hage">Hage</a></sup></h1><h1 id="网络中心度量"><a href="#网络中心度量" class="headerlink" title="网络中心度量"></a>网络中心度量</h1><p>在大数据量下，经典的Closeness Centrality和Betwenness Centrality几乎都是不可计算的。我认为，在大数据的前提下，应该定义一些适合大规模计算的新的Centrality。</p>
<h2 id="度中心性（Degree-Centrality）Freeman"><a href="#度中心性（Degree-Centrality）Freeman" class="headerlink" title="度中心性（Degree Centrality）Freeman"></a>度中心性（Degree Centrality）<sup><a href="#fn_Freeman" id="reffn_Freeman">Freeman</a></sup></h2><p>度中心性（Degree Centrality）是在网络分析中刻画节点中心性（Centrality）的最直接度量指标。一个节点的节点度越大就意味着这个节点的度中心性越高，该节点在网络中就越重要。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C_&#123;D&#125;\left(N_&#123;i&#125;\right)&#x3D;\sum_&#123;J&#x3D;1&#125;^&#123;g&#125; x_&#123;i j&#125;(i \neq j)</span><br></pre></td></tr></table></figure>
<p>其中$C<em>{D}\left(N</em>{i}\right)$表示节点i的度中心度，$\sum<em>{J=1}^{g} x</em>{i j}$用于计算节点i与其它g-1个j节点（i≠j，排除i与自身的联系；也就是说，主对角线的值可以忽略）之间的直接联系的数量。</p>
<h2 id="接近中心性（Closeness-Centrality）Freeman"><a href="#接近中心性（Closeness-Centrality）Freeman" class="headerlink" title="接近中心性（Closeness Centrality）Freeman"></a>接近中心性（Closeness Centrality）<sup><a href="#fn_Freeman" id="reffn_Freeman">Freeman</a></sup></h2><p>反映在网络中某一节点与其他节点之间的接近程度。如果节点到图中其它节点的最短距离都很小，那么我们认为该节点的Closeness Centrality高。</p>
<h2 id="中介中心性-中间中心性-Between-Centrality-Freeman"><a href="#中介中心性-中间中心性-Between-Centrality-Freeman" class="headerlink" title="中介中心性/中间中心性(Between Centrality) Freeman"></a>中介中心性/中间中心性(Between Centrality) <sup><a href="#fn_Freeman" id="reffn_Freeman">Freeman</a></sup></h2><p>以经过某个节点的最短路径数目来刻画节点重要性的指标。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C_&#123;B&#125;(v)&#x3D;\sum_&#123;s \neq v \neq t \in V&#125; \frac&#123;\sigma_&#123;s t&#125;(v)&#125;&#123;\sigma_&#123;s t&#125;&#125;</span><br></pre></td></tr></table></figure><br>其中，$\sigma<em>{st}$表示的是节点s和t之间的最短路径的数量，而$\sigma</em>{st}(v)$是最短路径中经过节点v的数量。</p>
<h2 id="特征向量中心性（Eigenvector-Centrality）Negre"><a href="#特征向量中心性（Eigenvector-Centrality）Negre" class="headerlink" title="特征向量中心性（Eigenvector Centrality）Negre"></a>特征向量中心性（Eigenvector Centrality）<sup><a href="#fn_Negre" id="reffn_Negre">Negre</a></sup></h2><p>一个节点的重要性既取决于其邻居节点的数量（即该节点的度），也取决于其邻居节点的重要性。</p>
<blockquote id="fn_1">
<sup>1</sup>. <a href="https://blog.csdn.net/wangjunliang/article/details/60468546">图论概念：Degree Centrality 和 Betweenness Centrality</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. <a href="https://baike.baidu.com/item/%E5%BA%A6%E4%B8%AD%E5%BF%83%E6%80%A7/17510724?fr=aladdin">度中心性</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_Freeman">
<sup>Freeman</sup>. L. C. Freeman, “Centrality in social networks conceptual clarification,” Social Networks, vol. 1, no. 3, pp. 215–239, 1978, doi: <a href="https://doi.org/10.1016/0378-8733(78)90021-7">https://doi.org/10.1016/0378-8733(78)90021-7</a>.<a href="#reffn_Freeman" title="Jump back to footnote [Freeman] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_Negre">
<sup>Negre</sup>. C. F. A. Negre et al., “Eigenvector centrality for characterization of protein allosteric pathways,” in Proceedings of the National Academy of Sciences, 2018, vol. 115(52), pp. E12201–E12208, doi: 10.1073/pnas.1810452115.<a href="#reffn_Negre" title="Jump back to footnote [Negre] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_Hage">
<sup>Hage</sup>. P. Hage and F. Harary, “Eccentricity and centrality in networks,” Social Networks, vol. 17, no. 1, pp. 57–63, 1995, doi: <a href="https://doi.org/10.1016/0378-8733(94)00248-9">https://doi.org/10.1016/0378-8733(94)00248-9</a>.<a href="#reffn_Hage" title="Jump back to footnote [Hage] in the text."> &#8617;</a>
</blockquote>
]]></content>
      <categories>
        <category>缺陷预测</category>
      </categories>
      <tags>
        <tag>缺陷预测</tag>
        <tag>软件度量</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas数据合并</title>
    <url>/archives/8d8b7079.html</url>
    <content><![CDATA[<p>Pandas包的merge、join、concat方法可以完成数据的合并和拼接，merge方法主要基于两个dataframe的共同列进行合并，join方法主要基于两个dataframe的索引进行合并，concat方法是对series或dataframe进行行拼接或列拼接。<br><a id="more"></a></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.cnblogs.com/keye/p/10791705.html">pandas-数据的合并与拼接
</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas数据操作</title>
    <url>/archives/c574eed0.html</url>
    <content><![CDATA[<h1 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h1><ul>
<li>字符串类型：object</li>
<li>整数类型：Int64，Int32，Int16, Int8 </li>
<li>无符号整数：UInt64，UInt32，UInt16, UInt8 </li>
<li>浮点数类型：float64，float32</li>
<li>日期和时间类型：datetime64[ns]、datetime64[ns, tz]、timedelta[ns]</li>
<li>布尔类型：bool</li>
</ul>
<h1 id="DataFrame列定义"><a href="#DataFrame列定义" class="headerlink" title="DataFrame列定义"></a>DataFrame列定义</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>columns = [<span class="string">&#x27;absolute_name&#x27;</span>,<span class="string">&#x27;version&#x27;</span>,<span class="string">&#x27;all_content&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df = pd.DataFrame(columns=columns)</span><br></pre></td></tr></table></figure>
<h2 id="写入"><a href="#写入" class="headerlink" title="写入"></a>写入</h2><h3 id="DataFrame插入行"><a href="#DataFrame插入行" class="headerlink" title="DataFrame插入行"></a>DataFrame插入行</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    df.loc[i] = [randint(-<span class="number">1</span>,<span class="number">1</span>) <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df = df.append(&#123;<span class="string">&#x27;absolute_name&#x27;</span>:<span class="number">1</span>,<span class="string">&#x27;version&#x27;</span>:<span class="number">2</span>,<span class="string">&#x27;all_content&#x27;</span>:<span class="number">3</span>&#125;,ignore_index=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="DataFrame插入列"><a href="#DataFrame插入列" class="headerlink" title="DataFrame插入列"></a>DataFrame插入列</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># 在数据框最后加上score一列，元素值分别为：80，98，67，90</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df1[<span class="string">&#x27;score&#x27;</span>]=[<span class="number">80</span>,<span class="number">98</span>,<span class="number">67</span>,<span class="number">90</span>,<span class="number">50</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    df.loc[i] = [randint(-<span class="number">1</span>,<span class="number">1</span>) <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.append([[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>col_name.insert(<span class="number">2</span>,<span class="string">&#x27;city&#x27;</span>)                      <span class="comment"># 在列索引为2的位置插入一列,列名为:city，刚插入时不会有值，整列都是NaN</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df1=df1.reindex(columns=col_name)              <span class="comment"># DataFrame.reindex() 对原行/列索引重新构建索引值</span></span><br></pre></td></tr></table></figure>
<h3 id="修改值"><a href="#修改值" class="headerlink" title="修改值"></a>修改值</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># loc和iloc 可以更换单行、单列、多行、多列的值,可直接添加列</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df1.loc[<span class="number">0</span>,<span class="string">&#x27;age&#x27;</span>]=<span class="number">25</span>      <span class="comment"># 思路：先用loc找到要更改的值，再用赋值（=）的方法实现更换值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df1.iloc[<span class="number">0</span>,<span class="number">2</span>]=<span class="number">25</span>         <span class="comment"># iloc：用索引位置来查找</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># at 、iat只能更换单个值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df1.at[<span class="number">0</span>,<span class="string">&#x27;age&#x27;</span>]=<span class="number">25</span>      <span class="comment"># iat 用来取某个单值,参数只能用数字索引</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df1.iat[<span class="number">0</span>,<span class="number">2</span>]=<span class="number">25</span>         <span class="comment"># at 用来取某个单值,参数只能用index和columns索引名称</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(df1)</span><br></pre></td></tr></table></figure>
<h2 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h2><h3 id="读取行"><a href="#读取行" class="headerlink" title="读取行"></a>读取行</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取行</span></span><br><span class="line">df[<span class="number">0</span>:<span class="number">16</span>]</span><br><span class="line">df[<span class="number">0</span>:<span class="number">1</span>]</span><br><span class="line"><span class="comment">#iloc只能用数字索引，不能用索引名</span></span><br><span class="line">df.iloc[<span class="number">0</span>:<span class="number">2</span>]<span class="comment">#前2行</span></span><br><span class="line">df.iloc[<span class="number">0</span>]<span class="comment">#第0行</span></span><br><span class="line">df.iloc[<span class="number">0</span>:<span class="number">2</span>,<span class="number">0</span>:<span class="number">2</span>]<span class="comment">#0、1行，0、1列</span></span><br><span class="line">df.iloc[[<span class="number">0</span>,<span class="number">2</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]]<span class="comment">#第0、2行，1、2、3列</span></span><br></pre></td></tr></table></figure>
<h3 id="读取部分列"><a href="#读取部分列" class="headerlink" title="读取部分列"></a>读取部分列</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -------------版本更新后不可用------------------------</span></span><br><span class="line"><span class="comment">#ix可以用数字索引，也可以用index和column索引</span></span><br><span class="line"><span class="comment"># df.ix[0]#取第0行</span></span><br><span class="line"><span class="comment"># df.ix[0:1]#取第0行</span></span><br><span class="line"><span class="comment"># df.ix[&#x27;one&#x27;:&#x27;two&#x27;]#取one、two行</span></span><br><span class="line"><span class="comment"># df.ix[0:2,0]#取第0、1行，第0列</span></span><br><span class="line"><span class="comment"># df.ix[0:1,&#x27;a&#x27;]#取第0行，a列</span></span><br><span class="line"><span class="comment"># df.ix[0:2,&#x27;a&#x27;:&#x27;c&#x27;]#取第0、1行，abc列</span></span><br><span class="line"><span class="comment"># df.ix[&#x27;one&#x27;:&#x27;two&#x27;,&#x27;a&#x27;:&#x27;c&#x27;]#取one、two行，abc列</span></span><br><span class="line"><span class="comment"># df.ix[0:2,0:1]#取第0、1行，第0列</span></span><br><span class="line"><span class="comment"># df.ix[0:2,0:2]#取第0、1行，第0、1列</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#loc只能通过index和columns来取，不能用数字</span></span><br><span class="line">df.loc[<span class="string">&#x27;one&#x27;</span>,<span class="string">&#x27;a&#x27;</span>]<span class="comment">#one行，a列</span></span><br><span class="line">df.loc[<span class="string">&#x27;one&#x27;</span>:<span class="string">&#x27;two&#x27;</span>,<span class="string">&#x27;a&#x27;</span>]<span class="comment">#one到two行，a列</span></span><br><span class="line">df.loc[<span class="string">&#x27;one&#x27;</span>:<span class="string">&#x27;two&#x27;</span>,<span class="string">&#x27;a&#x27;</span>:<span class="string">&#x27;c&#x27;</span>]<span class="comment">#one到two行，a到c列</span></span><br><span class="line">df.loc[<span class="string">&#x27;one&#x27;</span>:<span class="string">&#x27;two&#x27;</span>,[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]]<span class="comment">#one到two行，ac列</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#iat取某个单值,只能数字索引</span></span><br><span class="line">df.iat[<span class="number">1</span>,<span class="number">1</span>]<span class="comment">#第1行，1列</span></span><br><span class="line"><span class="comment">#at取某个单值,只能index和columns索引</span></span><br><span class="line">df.at[<span class="string">&#x27;one&#x27;</span>,<span class="string">&#x27;a&#x27;</span>]<span class="comment">#one行，a列</span></span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://blog.csdn.net/sinat_30353259/article/details/83818646">pandas apply应用并行进程，多核加快运行速度</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/100064394">Pandas教程 | 数据处理三板斧——map、apply、applymap详解</a></li>
<li><a href="https://www.cnblogs.com/nxf-rabbit75/p/10105271.html">pandas取dataframe特定行/列</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>python数据库操作</title>
    <url>/archives/7157f24b.html</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Python</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>python数据集划分</title>
    <url>/archives/501612f1.html</url>
    <content><![CDATA[<h1 id="保持标签比例不变"><a href="#保持标签比例不变" class="headerlink" title="保持标签比例不变"></a>保持标签比例不变</h1><h2 id="StratifiedKFold"><a href="#StratifiedKFold" class="headerlink" title="StratifiedKFold"></a>StratifiedKFold</h2><p>按顺序划分<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>skf = StratifiedKFold(n_splits=<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> train_index,test_index <span class="keyword">in</span> skf.split(<span class="built_in">range</span>(codelen),labels):</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    print(train_index,test_index)</span><br></pre></td></tr></table></figure></p>
<h2 id="StratifiedShuffleSplit"><a href="#StratifiedShuffleSplit" class="headerlink" title="StratifiedShuffleSplit"></a>StratifiedShuffleSplit</h2><p>打乱顺序<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedShuffleSplit</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>skf = StratifiedShuffleSplit(n_splits=<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> train_index,test_index <span class="keyword">in</span> skf.split(<span class="built_in">range</span>(codelen),labels):</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    print(train_index,test_index)</span><br></pre></td></tr></table></figure></p>
<h1 id="随机划分"><a href="#随机划分" class="headerlink" title="随机划分"></a>随机划分</h1><h1 id="train-test-split"><a href="#train-test-split" class="headerlink" title="train_test_split"></a>train_test_split</h1><p>按比例划分<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X, y = np.arange(<span class="number">10</span>).reshape((<span class="number">5</span>, <span class="number">2</span>)), <span class="built_in">range</span>(<span class="number">5</span>)</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">       [<span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>split</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch中conv1d和conv2d详解</title>
    <url>/archives/c312fd52.html</url>
    <content><![CDATA[<h1 id="conv2d"><a href="#conv2d" class="headerlink" title="conv2d"></a>conv2d</h1><p>In the simplest case, the output value of the layer with input size $(N, C<em>{in}, H, W)$ and output $(N,C</em>{out},H<em>{out},W</em>{out})$ can be precisely described as:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\operatorname&#123;out&#125;\left(N_&#123;i&#125;, C_&#123;\text &#123;out &#125;_&#123;j&#125;&#125;\right)&#x3D;\operatorname&#123;bias&#125;\left(C_&#123;\text &#123;out &#125;_&#123;j&#125;&#125;\right)+\sum_&#123;k&#x3D;0&#125;^&#123;C_&#123;\text &#123;in &#125;&#125;-1&#125; \text &#123; weight &#125;\left(C_&#123;\text &#123;out &#125;_&#123;j&#125;&#125;, k\right) \star \operatorname&#123;input&#125;\left(N_&#123;i&#125;, k\right)</span><br></pre></td></tr></table></figure><br>实例<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># With square kernels and equal stride</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, <span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># non-square kernels and unequal stride and with padding</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, (<span class="number">3</span>, <span class="number">5</span>), stride=(<span class="number">2</span>, <span class="number">1</span>), padding=(<span class="number">4</span>, <span class="number">2</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># non-square kernels and unequal stride and with padding and dilation</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, (<span class="number">3</span>, <span class="number">5</span>), stride=(<span class="number">2</span>, <span class="number">1</span>), padding=(<span class="number">4</span>, <span class="number">2</span>), dilation=(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>, <span class="number">100</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="groups"><a href="#groups" class="headerlink" title="groups"></a>groups</h2><p>groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,</p>
<ul>
<li>At groups=1, all inputs are convolved to all outputs.</li>
<li>At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.</li>
<li>At groups= in_channels, each input channel is convolved with its own set of filters (of size<br>$\frac{\text{out_channels}}{\text{in_channels}}$).</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nn.Conv2d(self.IN_CHANNEL, self.IN_CHANNEL*self.FILTER_NUM[i], (self.FILTERS[i], self.WORD_DIM),groups=self.IN_CHANNEL)</span><br><span class="line"><span class="comment"># 增加卷积层多样性</span></span><br></pre></td></tr></table></figure>
<h1 id="conv1d"><a href="#conv1d" class="headerlink" title="conv1d"></a>conv1d</h1><p>In the simplest case, the output value of the layer with input size $(N, C<em>{\text{in}}, L)$ and output $(N, C</em>{\text{out}}, L_{\text{out}})$ can be precisely described as:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\operatorname&#123;out&#125;\left(N_&#123;i&#125;, C_&#123;\text &#123;out &#125;_&#123;j&#125;&#125;\right)&#x3D;\operatorname&#123;bias&#125;\left(C_&#123;\text &#123;out &#125;_&#123;j&#125;&#125;\right)+\sum_&#123;k&#x3D;0&#125;^&#123;C_&#123;i n&#125;-1&#125; \text &#123; weight &#125;\left(C_&#123;\text &#123;out &#125;_&#123;j&#125;&#125;, k\right) \star \operatorname&#123;input&#125;\left(N_&#123;i&#125;, k\right)</span><br></pre></td></tr></table></figure><br>实例<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Conv1d(<span class="number">16</span>, <span class="number">33</span>, <span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Python</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytroch</tag>
        <tag>conv1d</tag>
        <tag>conv2d</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch处理不同长度序列</title>
    <url>/archives/e9de335f.html</url>
    <content><![CDATA[<a id="more"></a>
<h1 id="pad-sequence"><a href="#pad-sequence" class="headerlink" title="pad_sequence"></a>pad_sequence</h1><p>使用pad_squence将长度不同的序列补齐到统一长度<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.ones(<span class="number">25</span>, <span class="number">300</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.ones(<span class="number">22</span>, <span class="number">300</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = torch.ones(<span class="number">15</span>, <span class="number">300</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pad_sequence([a, b, c],padding_value=<span class="number">0.0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># size:25,3,300</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pad_sequence([a, b, c],batch_first=<span class="literal">True</span>,padding_value=<span class="number">0.0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># size:3,25,300</span></span><br></pre></td></tr></table></figure></p>
<h1 id="pack-padded-sequence-pad-packed-sequence"><a href="#pack-padded-sequence-pad-packed-sequence" class="headerlink" title="pack_padded_sequence, pad_packed_sequence"></a>pack_padded_sequence, pad_packed_sequence</h1><p>为了更高效的进行batch处理，用于rnn和lstm等<br>pack_padded_sequence将序列进行“压缩”<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence, pad_packed_sequence</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>seq = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>], [<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lens = [<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>packed = pack_padded_sequence(seq, lens, batch_first=<span class="literal">True</span>, enforce_sorted=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>packed</span><br><span class="line">PackedSequence(data=tensor([<span class="number">4</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">6</span>]), batch_sizes=tensor([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]),</span><br><span class="line">               sorted_indices=tensor([<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>]), unsorted_indices=tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>seq_unpacked</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lens_unpacked</span><br><span class="line">tensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.jianshu.com/p/376c16b71130">使用pytorch处理不同长度序列</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/342685890">pack_padded_sequence 和 pad_packed_sequence</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch常用函数学习</title>
    <url>/archives/dc88081e.html</url>
    <content><![CDATA[<h1 id="torch-sigmoid"><a href="#torch-sigmoid" class="headerlink" title="torch.sigmoid()"></a>torch.sigmoid()</h1><p>大于0则大于0.5，小于0则小于0.5<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\text &#123; out &#125;_&#123;i&#125;&#x3D;\frac&#123;1&#125;&#123;1+e^&#123;-\text &#123;input &#125;_&#123;i&#125;&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="F-log-softmax和F-softmax"><a href="#F-log-softmax和F-softmax" class="headerlink" title="F.log_softmax和F.softmax"></a>F.log_softmax和F.softmax</h1><h2 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h2><p>公式如下，一行和为1，输出是概率分布<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\frac&#123;\exp \left(x_&#123;i&#125;\right)&#125;&#123;\sum_&#123;j&#125; \exp \left(x_&#123;j&#125;\right)&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="log-softmax"><a href="#log-softmax" class="headerlink" title="log_softmax"></a>log_softmax</h2><p>等价于log(softmax(x)),值全部为负。</p>
]]></content>
      <categories>
        <category>Python</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch维度变换</title>
    <url>/archives/fb2b1394.html</url>
    <content><![CDATA[<h1 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h1><p>pytorch维度变换常用函数有view，unsqueeze，squeeze</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>va=torch.rand(<span class="number">4</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ve=va.unsqueeze(-<span class="number">1</span>)<span class="comment">#在最后一维后面增加一个维度</span></span><br></pre></td></tr></table></figure>
<h1 id="数据合并"><a href="#数据合并" class="headerlink" title="数据合并"></a>数据合并</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A=torch.ones(<span class="number">2</span>,<span class="number">3</span>) <span class="comment">#2x3的张量（矩阵）    </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B=<span class="number">2</span>*torch.ones(<span class="number">4</span>,<span class="number">3</span>)<span class="comment">#4x3的张量（矩阵）   </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C=torch.cat((A,B),<span class="number">0</span>)<span class="comment">#按维数0（行）拼接 </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>E=torch.cat([A,B,C],axis=<span class="number">0</span>)<span class="comment">#按维数0（行）拼接 </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>D=<span class="number">2</span>*torch.ones(<span class="number">2</span>,<span class="number">4</span>) <span class="comment">#2x4的张量（矩阵）</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C=torch.cat((A,D),<span class="number">1</span>)<span class="comment">#按维数1（列）拼接</span></span><br></pre></td></tr></table></figure>
<h1 id="常用操作对维度的影响"><a href="#常用操作对维度的影响" class="headerlink" title="常用操作对维度的影响"></a>常用操作对维度的影响</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x=[</span><br><span class="line">[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],[<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]],</span><br><span class="line">[[<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>],[<span class="number">17</span>,<span class="number">18</span>,<span class="number">19</span>,<span class="number">20</span>],[<span class="number">21</span>,<span class="number">22</span>,<span class="number">23</span>,<span class="number">24</span>]]</span><br><span class="line">]</span><br><span class="line">x=torch.tensor(x).<span class="built_in">float</span>()</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">print(<span class="string">&quot;shape of x:&quot;</span>)  <span class="comment">##[2,3,4]                                                                      </span></span><br><span class="line">print(x.shape)                                                                                   </span><br><span class="line"><span class="comment">#</span></span><br><span class="line">print(<span class="string">&quot;shape of x.mean(axis=0,keepdim=True):&quot;</span>)          <span class="comment">#[1, 3, 4]</span></span><br><span class="line">print(x.mean(axis=<span class="number">0</span>,keepdim=<span class="literal">True</span>).shape)                       </span><br><span class="line"><span class="comment">#[[[ 7.,  8.,  9., 10.],</span></span><br><span class="line"><span class="comment">#         [11., 12., 13., 14.],</span></span><br><span class="line"><span class="comment">#         [15., 16., 17., 18.]]]</span></span><br><span class="line">print(<span class="string">&quot;shape of x.mean(axis=0,keepdim=False):&quot;</span>)         <span class="comment">#[3, 4]</span></span><br><span class="line">print(x.mean(axis=<span class="number">0</span>,keepdim=<span class="literal">False</span>).shape)                     </span><br><span class="line"><span class="comment">#</span></span><br><span class="line">print(<span class="string">&quot;shape of x.mean(axis=1,keepdim=True):&quot;</span>)          <span class="comment">#[2, 1, 4]</span></span><br><span class="line">print(x.mean(axis=<span class="number">1</span>,keepdim=<span class="literal">True</span>).shape)                      </span><br><span class="line"><span class="comment">#</span></span><br><span class="line">print(<span class="string">&quot;shape of x.mean(axis=1,keepdim=False):&quot;</span>)         <span class="comment">#[2, 4]</span></span><br><span class="line">print(x.mean(axis=<span class="number">1</span>,keepdim=<span class="literal">False</span>).shape)                    </span><br><span class="line"><span class="comment">#[[ 5.,  6.,  7.,  8.],</span></span><br><span class="line"><span class="comment">#        [17., 18., 19., 20.]]</span></span><br><span class="line">print(<span class="string">&quot;shape of x.mean(axis=2,keepdim=True):&quot;</span>)          <span class="comment">#[2, 3, 1]</span></span><br><span class="line">print(x.mean(axis=<span class="number">2</span>,keepdim=<span class="literal">True</span>).shape)                     </span><br><span class="line"><span class="comment">#</span></span><br><span class="line">print(<span class="string">&quot;shape of x.mean(axis=2,keepdim=False):&quot;</span>)         <span class="comment">#[2, 3]</span></span><br><span class="line">print(x.mean(axis=<span class="number">2</span>,keepdim=<span class="literal">False</span>).shape)</span><br><span class="line"><span class="comment">#[[ 2.5000,  6.5000, 10.5000],</span></span><br><span class="line"><span class="comment">#        [14.5000, 18.5000, 22.5000]]</span></span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://blog.csdn.net/weixin_30972263/article/details/113680673">pytorch增加一维_Pytorch学习笔记3：维度变换测试代码</a></li>
<li><a href="https://blog.csdn.net/qq_36810398/article/details/104845401">pytorch中tensor.mean(axis, keepdim)参数理解小实验</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch自定义损失</title>
    <url>/archives/71b70985.html</url>
    <content><![CDATA[<h1 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ContrastiveLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Contrastive loss</span></span><br><span class="line"><span class="string">    Takes embeddings of two samples and a target label == 1 if samples are from the same class and label == 0 otherwise</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, margin</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ContrastiveLoss, self).__init__()</span><br><span class="line">        self.margin = margin</span><br><span class="line">        self.eps = <span class="number">1e-9</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, output1, output2, target, size_average=<span class="literal">True</span></span>):</span></span><br><span class="line">        distances = (output2 - output1).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">1</span>)  <span class="comment"># squared distances</span></span><br><span class="line">        losses = <span class="number">0.5</span> * (target.<span class="built_in">float</span>() * distances +</span><br><span class="line">                        (<span class="number">1</span> + -<span class="number">1</span> * target).<span class="built_in">float</span>() * F.relu(self.margin - (distances + self.eps).sqrt()).<span class="built_in">pow</span>(<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">return</span> losses.mean() <span class="keyword">if</span> size_average <span class="keyword">else</span> losses.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TripletLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Triplet loss</span></span><br><span class="line"><span class="string">    Takes embeddings of an anchor sample, a positive sample and a negative sample</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, margin</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TripletLoss, self).__init__()</span><br><span class="line">        self.margin = margin</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, anchor, positive, negative, size_average=<span class="literal">True</span></span>):</span></span><br><span class="line">        distance_positive = (anchor - positive).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">1</span>)  <span class="comment"># .pow(.5)</span></span><br><span class="line">        distance_negative = (anchor - negative).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">1</span>)  <span class="comment"># .pow(.5)</span></span><br><span class="line">        losses = F.relu(distance_positive - distance_negative + self.margin)</span><br><span class="line">        <span class="keyword">return</span> losses.mean() <span class="keyword">if</span> size_average <span class="keyword">else</span> losses.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<h1 id="为可学习参数施加约束或正则项"><a href="#为可学习参数施加约束或正则项" class="headerlink" title="为可学习参数施加约束或正则项"></a>为可学习参数施加约束或正则项</h1><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li><a href="https://blog.csdn.net/u011501388/article/details/100016577">在PyTorch中为可学习参数施加约束或正则项的方法</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>损失</tag>
      </tags>
  </entry>
  <entry>
    <title>词嵌入</title>
    <url>/archives/8b0b64bb.html</url>
    <content><![CDATA[<h1 id="预训练好的词嵌入"><a href="#预训练好的词嵌入" class="headerlink" title="预训练好的词嵌入"></a>预训练好的词嵌入</h1><h2 id="spcy"><a href="#spcy" class="headerlink" title="spcy"></a>spcy</h2><p><a href="https://spacy.io/models/en#en_core_web_lg">en_core_web_lg</a></p>
<h2 id="gensim"><a href="#gensim" class="headerlink" title="gensim"></a>gensim</h2>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>词嵌入</tag>
      </tags>
  </entry>
  <entry>
    <title>gensim版本更新导致的api使用变更</title>
    <url>/archives/eef78ec0.html</url>
    <content><![CDATA[<p>gensim版本更新导致api使用有变更<br><a id="more"></a></p>
<h1 id="4-1-2"><a href="#4-1-2" class="headerlink" title="4.1.2"></a>4.1.2</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word2vec = Word2Vec.load(config.data_path + <span class="string">&quot;word_w2v_128&quot;</span>).wv</span><br><span class="line">word2vec.vectors: ndarray</span><br><span class="line">word2vec.index_to_key: <span class="built_in">list</span></span><br><span class="line"><span class="comment"># 等价于之前的vocab</span></span><br><span class="line">word2vec.key_to_index: <span class="built_in">dict</span></span><br></pre></td></tr></table></figure>
<h1 id=""><a href="#" class="headerlink" title="*"></a><em>*</em></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word2vec = Word2Vec.load(config.data_path + <span class="string">&quot;word_w2v_128&quot;</span>).wv</span><br><span class="line">word2vec.syn0: ndarray</span><br><span class="line">word2vec.index_to_key: <span class="built_in">list</span></span><br><span class="line">word2vec.key_to_index: <span class="built_in">dict</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>gensim</tag>
      </tags>
  </entry>
  <entry>
    <title>bert系列(一)</title>
    <url>/archives/3750985.html</url>
    <content><![CDATA[<h1 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input_ids = tokenizer.encode(<span class="string">&#x27;hello world bert!&#x27;</span>)</span><br><span class="line">[<span class="number">101</span>, <span class="number">7592</span>, <span class="number">2088</span>, <span class="number">14324</span>, <span class="number">999</span>, <span class="number">102</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ids = torch.LongTensor(input_ids)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = tokenizer.convert_ids_to_tokens(input_ids)</span><br><span class="line">[<span class="string">&#x27;[CLS]&#x27;</span>, <span class="string">&#x27;hello&#x27;</span>, <span class="string">&#x27;world&#x27;</span>, <span class="string">&#x27;bert&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;[SEP]&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>model = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>, output_hidden_states=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>model = model.to(device)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ids = ids.to(device)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>granola_ids = ids.unsqueeze(<span class="number">0</span>)</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">#In the example below, an additional argument has been given to the model initialisation. output_hidden_states will give us more output information. By default, a BertModel will return a tuple but the contents of that tuple differ depending on the configuration of the model. When passing output_hidden_states=True, the tuple will contain (in order; shape in brackets):</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">#1. the last hidden state (batch_size, sequence_length, hidden_size)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">#1. the pooler_output of the classification token (batch_size, hidden_size)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">#1. the hidden_states of the outputs of the model at each layer and the initial embedding outputs (batch_size, sequence_length, hidden_size)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = model(input_ids=granola_ids) <span class="comment"># tuple</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>hidden_states = out[<span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">&quot;last hidden state:&quot;</span>,out[<span class="number">0</span>].shape) <span class="comment">#torch.Size([1, 6, 768])</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">&quot;pooler_output of classification token:&quot;</span>,out[<span class="number">1</span>].shape)<span class="comment">#[1,768] cls</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">&quot;all hidden_states:&quot;</span>, <span class="built_in">len</span>(out[<span class="number">2</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i, each_layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(hidden_states):</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    print(<span class="string">&#x27;layer=&#x27;</span>,i, each_layer)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sentence_embedding = torch.mean(hidden_states[-<span class="number">1</span>], dim=<span class="number">1</span>).squeeze()</span><br><span class="line">torch.Size([<span class="number">768</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># get last four layers</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>last_four_layers = [hidden_states[i] <span class="keyword">for</span> i <span class="keyword">in</span> (-<span class="number">1</span>, -<span class="number">2</span>, -<span class="number">3</span>, -<span class="number">4</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># cast layers to a tuple and concatenate over the last dimension</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cat_hidden_states = torch.cat(<span class="built_in">tuple</span>(last_four_layers), dim=-<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># take the mean of the concatenated vector over the token dimension</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cat_sentence_embedding = torch.mean(cat_hidden_states, dim=<span class="number">1</span>).squeeze()</span><br></pre></td></tr></table></figure>
<h1 id="tokenizer"><a href="#tokenizer" class="headerlink" title="tokenizer"></a>tokenizer</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">&quot;词典大小:&quot;</span>,tokenizer.vocab_size)</span><br><span class="line">词典大小: <span class="number">30522</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = <span class="string">&quot;the game has gone!unaffable  I have a new GPU!&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokens = tokenizer.tokenize(text)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">&quot;英文分词来一个：&quot;</span>,tokens)</span><br><span class="line">英文分词来一个： [<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;game&#x27;</span>, <span class="string">&#x27;has&#x27;</span>, <span class="string">&#x27;gone&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;una&#x27;</span>, <span class="string">&#x27;##ffa&#x27;</span>, <span class="string">&#x27;##ble&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;have&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;new&#x27;</span>, <span class="string">&#x27;gp&#x27;</span>, <span class="string">&#x27;##u&#x27;</span>, <span class="string">&#x27;!&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = <span class="string">&quot;我爱北京天安门，吢吣&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokens = tokenizer.tokenize(text)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">&quot;中文分词来一个：&quot;</span>,tokens)</span><br><span class="line">中文分词来一个： [<span class="string">&#x27;我&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>, <span class="string">&#x27;北&#x27;</span>, <span class="string">&#x27;京&#x27;</span>, <span class="string">&#x27;天&#x27;</span>, <span class="string">&#x27;安&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>, <span class="string">&#x27;，&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input_ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">&quot;id-token转换:&quot;</span>,input_ids)</span><br><span class="line"><span class="built_in">id</span>-token转换: [<span class="number">1855</span>, <span class="number">100</span>, <span class="number">1781</span>, <span class="number">1755</span>, <span class="number">1811</span>, <span class="number">1820</span>, <span class="number">100</span>, <span class="number">1989</span>, <span class="number">100</span>, <span class="number">100</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = tokenizer.convert_ids_to_tokens(input_ids)</span><br><span class="line">[<span class="string">&#x27;我&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>, <span class="string">&#x27;北&#x27;</span>, <span class="string">&#x27;京&#x27;</span>, <span class="string">&#x27;天&#x27;</span>, <span class="string">&#x27;安&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>, <span class="string">&#x27;，&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sen_code = tokenizer.encode_plus(<span class="string">&quot;i like  you  much&quot;</span>, <span class="string">&quot;but not him&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">&quot;多句子encode：&quot;</span>,sen_code)</span><br><span class="line">多句子encode： &#123;<span class="string">&#x27;input_ids&#x27;</span>: [<span class="number">101</span>, <span class="number">1045</span>, <span class="number">2066</span>, <span class="number">2017</span>, <span class="number">2172</span>, <span class="number">102</span>, <span class="number">2021</span>, <span class="number">2025</span>, <span class="number">2032</span>, <span class="number">102</span>], <span class="string">&#x27;token_type_ids&#x27;</span>: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], <span class="string">&#x27;attention_mask&#x27;</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer.encode(<span class="string">&quot;i like  you  much&quot;</span>,add_special_tokens=<span class="literal">False</span>)<span class="comment"># 不添加首尾</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">&quot;decode：&quot;</span>,tokenizer.decode(sen_code[<span class="string">&#x27;input_ids&#x27;</span>]))</span><br><span class="line">decode： [CLS] i like you much [SEP] but <span class="keyword">not</span> him [SEP]</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://blog.csdn.net/iterate7/article/details/108921916">bert系列第一篇： bert进行embedding</a></li>
<li><a href="https://blog.csdn.net/iterate7/article/details/108959082">bert第三篇：tokenizer</a></li>
<li><a href="https://github.com/DA-southampton/Read_Bert_Code">Bert源码阅读与讲解(Pytorch版本)</a></li>
</ol>
]]></content>
      <categories>
        <category>pytorch</category>
        <category>bert</category>
      </categories>
      <tags>
        <tag>bert</tag>
        <tag>embedding</tag>
        <tag>tokenizer</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas加进度条</title>
    <url>/archives/ae25239b.html</url>
    <content><![CDATA[<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.cnpython.com/qa/53145">与Pandas一起使用TQDM进度条</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>pytorch预处理</title>
    <url>/archives/20424ced.html</url>
    <content><![CDATA[<h2 id="建立词表"><a href="#建立词表" class="headerlink" title="建立词表"></a>建立词表</h2><p>使用build_vocab_from_iterator函数<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">#generating vocab from text file</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> io</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> build_vocab_from_iterator</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">yield_tokens</span>(<span class="params">file_path</span>):</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="keyword">with</span> io.<span class="built_in">open</span>(file_path, encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>            <span class="keyword">yield</span> line.strip().split()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Define special symbols and indices</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>UNK_IDX, PAD_IDX = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Make sure the tokens are in order of their indices to properly insert them in vocab</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>special_symbols = [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">#tokens = [&#x27;abcde&#x27;,&#x27;fghij&#x27;,&#x27;lmnop&#x27;,&#x27;qrst&#x27;] # [[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;],[&#x27;d&#x27;,&#x27;e&#x27;]]</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocab = build_vocab_from_iterator(yield_tokens(train_pair),min_freq=<span class="number">1</span>,specials=special_symbols,special_first=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocab_size = <span class="built_in">len</span>(vocab) <span class="comment"># vocab大小       </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocab.set_default_index(UNK_IDX)</span><br></pre></td></tr></table></figure></p>
<h2 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">min_freq = <span class="number">5</span></span><br><span class="line">special_tokens = [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line"></span><br><span class="line">vocab = torchtext.vocab.build_vocab_from_iterator(train_data[<span class="string">&#x27;tokens&#x27;</span>],</span><br><span class="line">                                                  min_freq=min_freq,</span><br><span class="line">                                                  specials=special_tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_data[&#x27;tokens&#x27;] is a list of a list of strings, i.e. [[&#x27;hello&#x27;, &#x27;world&#x27;], [&#x27;goodbye&#x27;, &#x27;moon&#x27;]], where [&#x27;hello&#x27;, &#x27;moon&#x27;] is the tokens corresponding to the first example in the training set.</span></span><br><span class="line"></span><br><span class="line">pretrained_vectors = torchtext.vocab.FastText()</span><br><span class="line"></span><br><span class="line">pretrained_embedding = pretrained_vectors.get_vecs_by_tokens(vocab.get_itos())</span><br><span class="line"></span><br><span class="line"><span class="comment"># vocab.get_itos() returns a list of strings (tokens), where the token at the i&#x27;th position is what you get from doing vocab[token]</span></span><br><span class="line"><span class="comment"># get_vecs_by_tokens gets the pre-trained vector for each string when given a list of strings</span></span><br><span class="line"><span class="comment"># therefore pretrained_embedding is a fully &quot;aligned&quot; embedding matrix</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NBoW</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, output_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.fc = nn.Linear(embedding_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        <span class="comment"># text = [batch size, seq len]</span></span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line">        <span class="comment"># embedded = [batch size, seq len, embedding dim]</span></span><br><span class="line">        pooled = embedded.mean(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># pooled = [batch size, embedding dim]</span></span><br><span class="line">        prediction = self.fc(pooled)</span><br><span class="line">        <span class="comment"># prediction = [batch size, output dim]</span></span><br><span class="line">        <span class="keyword">return</span> prediction</span><br><span class="line"></span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line">embedding_dim = <span class="number">300</span></span><br><span class="line">output_dim = n_classes</span><br><span class="line"></span><br><span class="line">model = NBoW(vocab_size, embedding_dim, output_dim, pad_index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># super basic model here, important thing is the nn.Embedding layer that needs to have an embedding layer that is initialized as nn.Embedding(vocab_size, embedding_dim) with embedding_dim = 300 as that&#x27;s the dimensions of the FastText embedding</span></span><br><span class="line"></span><br><span class="line">model.embedding.weight.data = pretrained_embedding</span><br><span class="line"></span><br><span class="line"><span class="comment"># overwrite the model&#x27;s initial embedding matrix weights with that of the pre-trained embeddings from FastText</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unk_init</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.randn_like(x)</span><br><span class="line"></span><br><span class="line">vectors = torchtext.vocab.FastText(unk_init=unk_init)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="torchtext"><a href="#torchtext" class="headerlink" title="torchtext"></a>torchtext</h1><p>之前的版本vectors是vocab的属性，可以用，最新版不行</p>
<h2 id="定义Field"><a href="#定义Field" class="headerlink" title="定义Field"></a>定义Field</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spacy_en = spacy.load(<span class="string">&#x27;en&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenizer</span>(<span class="params">text</span>):</span> <span class="comment"># create a tokenizer function</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义分词操作</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy_en.tokenizer(text)]</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">field在默认的情况下都期望一个输入是一组单词的序列，并且将单词映射成整数。</span></span><br><span class="line"><span class="string">这个映射被称为vocab。如果一个field已经被数字化了并且不需要被序列化，</span></span><br><span class="line"><span class="string">可以将参数设置为use_vocab=False以及sequential=False。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">LABEL = data.Field(sequential=<span class="literal">False</span>, use_vocab=<span class="literal">False</span>)</span><br><span class="line">TEXT = data.Field(sequential=<span class="literal">True</span>, tokenize=tokenizer, lower=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="建立vocab"><a href="#建立vocab" class="headerlink" title="建立vocab"></a>建立vocab</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TEXT.build_vocab(train, vectors=<span class="string">&#x27;glove.6B.100d&#x27;</span>)<span class="comment">#, max_size=30000)</span></span><br><span class="line"><span class="comment"># 当 corpus 中有的 token 在 vectors 中不存在时 的初始化方式.</span></span><br><span class="line">TEXT.vocab.vectors.unk_init = init.xavier_uniform</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://blog.csdn.net/JWoswin/article/details/92821752">Torchtext使用教程</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Torchtext使用教程</title>
    <url>/archives/7a364fd9.html</url>
    <content><![CDATA[<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://blog.csdn.net/leo_95/article/details/87708267">torchtext学习总结</a></li>
<li><a href="https://www.jianshu.com/p/71176275fdc5">Torchtext使用教程</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>torchtext</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu系统问题出错修复记录汇总</title>
    <url>/archives/3530b200.html</url>
    <content><![CDATA[<p>记录遇到的问题及修复记录<br><a id="more"></a></p>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><ol>
<li>grub-install: error: cannot find EFI directory<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt remove grub-efi-amd64-signed</span><br><span class="line">sudo apt install grub-pc</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://askubuntu.com/questions/1338497/grub-crashes-on-apt-upgrade-grub-install-error-cannot-find-efi-directory">https://askubuntu.com/questions/1338497/grub-crashes-on-apt-upgrade-grub-install-error-cannot-find-efi-directory</a></li>
</ol>
]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas分组与排序</title>
    <url>/archives/bd615f88.html</url>
    <content><![CDATA[<p>groupby,nlargest,nsmallest<br><a id="more"></a></p>
<h1 id="初始化数据"><a href="#初始化数据" class="headerlink" title="初始化数据"></a>初始化数据</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">company=[<span class="string">&quot;A&quot;</span>,<span class="string">&quot;B&quot;</span>,<span class="string">&quot;C&quot;</span>]</span><br><span class="line">data=pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&quot;company&quot;</span>:[company[x] <span class="keyword">for</span> x <span class="keyword">in</span> np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(company),<span class="number">10</span>)],</span><br><span class="line">    <span class="string">&quot;salary&quot;</span>:np.random.randint(<span class="number">5</span>,<span class="number">50</span>,<span class="number">10</span>),</span><br><span class="line">    <span class="string">&quot;age&quot;</span>:np.random.randint(<span class="number">15</span>,<span class="number">50</span>,<span class="number">10</span>)</span><br><span class="line">&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h1 id="选取最大或最小的前n个行"><a href="#选取最大或最小的前n个行" class="headerlink" title="选取最大或最小的前n个行"></a>选取最大或最小的前n个行</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; data.nlargest(3,&#39;salary&#39;)</span><br><span class="line">&gt;&gt;&gt; data.nsmallest(3,&#39;salary&#39;)</span><br></pre></td></tr></table></figure>
<h1 id="多重排序"><a href="#多重排序" class="headerlink" title="多重排序"></a>多重排序</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df.sort_values(by=[<span class="string">&quot;Worthy&quot;</span>,<span class="string">&quot;Price&quot;</span>],ascending=[<span class="literal">True</span>,<span class="literal">False</span>])</span><br></pre></td></tr></table></figure>
<h1 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h1><h2 id="分组排序"><a href="#分组排序" class="headerlink" title="分组排序"></a>分组排序</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; group &#x3D; data.groupby(&quot;company&quot;)</span><br><span class="line">&gt;&gt;&gt; group.apply(lambda x:x.nlargest(2,&#39;salary&#39;))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_oldest_staff(x):</span><br><span class="line">    df &#x3D; x.sort_values(by&#x3D;&#39;age&#39;, ascending&#x3D;True)</span><br><span class="line">    return df.iloc[-1, :]</span><br><span class="line">&gt;&gt;&gt; oldest_staff &#x3D; data.groupby(&#39;company&#39;,as_index&#x3D;False).apply(get_oldest_staff)</span><br><span class="line"># dataframe</span><br></pre></td></tr></table></figure>
<p>group.get_group()获取指定组<br>group.first()获取每组第一行<br>group.indices.keys()或者group.first().index获取分组具体值</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/101284491">Pandas教程 | 超好用的Groupby用法详解</a></li>
<li><a href="https://www.cnblogs.com/wqbin/p/11775812.html">pandas df 遍历行方法</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>分组与排序</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas条件筛选</title>
    <url>/archives/216ca791.html</url>
    <content><![CDATA[<p>pandas.where<br><a id="more"></a></p>
<h1 id="选取某个属性等于特定值的所有行记录"><a href="#选取某个属性等于特定值的所有行记录" class="headerlink" title="选取某个属性等于特定值的所有行记录"></a>选取某个属性等于特定值的所有行记录</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[(df[‘column_name’] == target_value)]</span><br></pre></td></tr></table></figure>
<h1 id="选取某个属性在指定列表中的所有行记录"><a href="#选取某个属性在指定列表中的所有行记录" class="headerlink" title="选取某个属性在指定列表中的所有行记录"></a>选取某个属性在指定列表中的所有行记录</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[(df[‘column_name’].isin(<span class="built_in">list</span>))]</span><br></pre></td></tr></table></figure>
<h1 id="模糊筛选"><a href="#模糊筛选" class="headerlink" title="模糊筛选"></a>模糊筛选</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df[<span class="string">&#x27;区域&#x27;</span>].<span class="built_in">str</span>.contains(<span class="string">&#x27;四川&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<h1 id="复杂条件筛选"><a href="#复杂条件筛选" class="headerlink" title="复杂条件筛选"></a>复杂条件筛选</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">flag = srcs.apply(<span class="keyword">lambda</span> x: x <span class="keyword">if</span> <span class="built_in">len</span>(x[<span class="string">&#x27;stemmed&#x27;</span>]) &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span>, axis=<span class="number">1</span>).dropna()</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1>]]></content>
      <categories>
        <category>Python</category>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>条件筛选</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch学习率预热</title>
    <url>/archives/75cc12c1.html</url>
    <content><![CDATA[<p>在torch.optim.lr_scheduler和transformers.get_linear_schedule_with_warmup<br><a id="more"></a></p>
<h1 id="get-linear-schedule-with-warmup"><a href="#get-linear-schedule-with-warmup" class="headerlink" title="get_linear_schedule_with_warmup"></a>get_linear_schedule_with_warmup</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># training steps 的数量: [number of batches] x [number of epochs].</span><br><span class="line">total_steps &#x3D; len(train_dataloader) * epochs</span><br><span class="line"> </span><br><span class="line"># 设计 learning rate scheduler</span><br><span class="line">scheduler &#x3D; get_linear_schedule_with_warmup(optimizer, num_warmup_steps &#x3D; 50, </span><br><span class="line">                                            num_training_steps &#x3D; total_steps)</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://blog.csdn.net/zisuina_2/article/details/103258573">PyTorch torch.optim.lr_scheduler 学习率 - LambdaLR;StepLR;MultiStepLR;ExponentialLR</a></li>
<li><a href="https://blog.csdn.net/Xiao_CangTian/article/details/109269555">学习率预热(transformers.get_linear_schedule_with_warmup)</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>学习率</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch数据拼接与拆分</title>
    <url>/archives/e89136dc.html</url>
    <content><![CDATA[<p>cat、stack、split、chunck<br><a id="more"></a></p>
<h1 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack"></a>torch.stack</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.rand(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.rand(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = torch.stack([a, b]) <span class="comment">#torch.Size([2, 2, 5])</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.stack([a, b], dim=<span class="number">1</span>)<span class="comment">#torch.Size([2, 2, 5])</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.stack([a, b], dim=<span class="number">2</span>) <span class="comment">#torch.Size([2, 5,2])</span></span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.cnblogs.com/jaysonteng/p/13038080.html">pytorch数据拼接与拆分</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>pytorch模型参数初始化</title>
    <url>/archives/d7fbb858.html</url>
    <content><![CDATA[<p>pytorch模型中有weight和bias参数，可进行初始化<br><a id="more"></a></p>
<h1 id="xavier初始化"><a href="#xavier初始化" class="headerlink" title="xavier初始化"></a>xavier初始化</h1><p>torch.nn.init.xavier_uniform(tensor, gain=1)</p>
<h1 id="kaiming初始化"><a href="#kaiming初始化" class="headerlink" title="kaiming初始化"></a>kaiming初始化</h1><p>torch.nn.init.kaiming_uniform(tensor, a=0, mode=’fan_in’)</p>
<h1 id=""><a href="#" class="headerlink" title=" "></a> </h1>]]></content>
      <categories>
        <category>Python</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>sklearn学习-特征提取</title>
    <url>/archives/66553acf.html</url>
    <content><![CDATA[<p>CountVectorizer，<br><a id="more"></a></p>
<h2 id="CountVectorizer"><a href="#CountVectorizer" class="headerlink" title="CountVectorizer"></a>CountVectorizer</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vectorizer = CountVectorizer()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>corpus = [</span><br><span class="line"><span class="meta">... </span>    <span class="string">&#x27;This is the first document.&#x27;</span>,</span><br><span class="line"><span class="meta">... </span>    <span class="string">&#x27;This is the second second document.&#x27;</span>,</span><br><span class="line"><span class="meta">... </span>    <span class="string">&#x27;And the third one.&#x27;</span>,</span><br><span class="line"><span class="meta">... </span>    <span class="string">&#x27;Is this the first document?&#x27;</span>,</span><br><span class="line"><span class="meta">... </span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = vectorizer.fit_transform(corpus)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X</span><br><span class="line">&lt;4x9 sparse matrix of <span class="built_in">type</span> <span class="string">&#x27;&lt;... &#x27;</span>numpy.int64<span class="string">&#x27;&gt;&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="number">19</span> stored elements <span class="keyword">in</span> Compressed Sparse ... <span class="built_in">format</span>&gt;</span><br></pre></td></tr></table></figure>
<p>词袋模型，统计的每个词的个数</p>
<h2 id="TfidfTransformer"><a href="#TfidfTransformer" class="headerlink" title="TfidfTransformer"></a>TfidfTransformer</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>transformer = TfidfTransformer(smooth_idf=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>counts = [[<span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line"><span class="meta">... </span>          [<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line"><span class="meta">... </span>          [<span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line"><span class="meta">... </span>          [<span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line"><span class="meta">... </span>          [<span class="number">3</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line"><span class="meta">... </span>          [<span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]]</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tfidf = transformer.fit_transform(counts)</span><br></pre></td></tr></table></figure>
<p><strong>参数解析</strong></p>
<ol>
<li>smooth_idf= False or True<script type="math/tex; mode=display">\operatorname{idf}(t)=\log \frac{n}{\operatorname{df}(t)}+1 \text{ or } \operatorname{idf}(t)=\log \frac{1+n}{1+\mathrm{df}(t)}+1</script></li>
<li>sublinear_tf=Fasle or True<script type="math/tex; mode=display">tf \text{ or } 1 + log(tf)</script><h2 id="TfidfVectorizer"><a href="#TfidfVectorizer" class="headerlink" title="TfidfVectorizer"></a>TfidfVectorizer</h2>TfidfVectorizer that combines all the options of CountVectorizer and TfidfTransformer in a single model<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vectorizer = TfidfVectorizer()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vectorizer.fit_transform(corpus)</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu安装java教程</title>
    <url>/archives/4134902e.html</url>
    <content><![CDATA[<a id="more"></a>
<h1 id="ubuntu"><a href="#ubuntu" class="headerlink" title="ubuntu"></a>ubuntu</h1><h2 id="Setting-the-Default-JDK"><a href="#Setting-the-Default-JDK" class="headerlink" title="Setting the Default JDK"></a>Setting the Default JDK</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.8.0_221/bin/java 1</span><br><span class="line">sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/jdk1.8.0_221/bin/javac 1</span><br></pre></td></tr></table></figure>
<p>To set a default JDK<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --config java</span><br></pre></td></tr></table></figure></p>
<h1 id="macos"><a href="#macos" class="headerlink" title="macos"></a>macos</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.linuxbabe.com/ubuntu/install-oracle-java-8-openjdk-11-ubuntu-18-04-18-10">How To Install Oracle Java 8 and OpenJDK 11 on Ubuntu 18.04, 20.04</a></li>
<li><a href="https://www.cnblogs.com/genghenggao/p/10313208.html">Ubuntu18.04 安装Tomcat 9</a></li>
<li><a href="https://blog.csdn.net/qq_25272679/article/details/80480500">tomcat如何添加用户信息</a></li>
<li><a href="https://blog.csdn.net/u010324331/article/details/105441518">Mac配置多版本java环境</a></li>
<li><a href="https://blog.csdn.net/weixin_42367233/article/details/114252997?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-1.no_search_link&amp;spm=1001.2101.3001.4242.2">mac 多java环境变量配置_java_Mac安装多个JDK版本并设置环境变量</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>ubuntu常用指令学习</title>
    <url>/archives/8f34d334.html</url>
    <content><![CDATA[<p>ubuntu常用指令学习：whereis、find、locate、which、grep<br><a id="more"></a></p>
<h1 id="查找指定内容"><a href="#查找指定内容" class="headerlink" title="查找指定内容"></a>查找指定内容</h1><h2 id="查找文件"><a href="#查找文件" class="headerlink" title="查找文件"></a>查找文件</h2><p><strong>whereis + 文件名</strong>：程序名搜索<br><strong>find / -name + 文件名</strong>: 指定的目录下遍历查找，如果目录使用 / 则表示在所有目录下查找，find方式查找文件消耗资源比较大，速度也慢一点。<br><strong>locate+文件名</strong>: linux会把系统内所有的文件都记录在一个数据库文件中，使用locate+文件名的方法会在linux系统维护的这个数据库中去查找目标，相比find命令去遍历磁盘查找的方式，效率会高很多，比较推荐使用这种方法。<br>但有一个问题是数据库文件不是实时更新的，一般会每周更新一次，所以使用locate命令查找到的结果不一定是准确的。当然可以在使用locate之前通过 updatedb 命令更新一次数据库，保证结果的性。<br><strong>which+可执行文件名</strong>: which的作用是在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。<br>使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。</p>
<h2 id="查找内容"><a href="#查找内容" class="headerlink" title="查找内容"></a>查找内容</h2><ol>
<li>grep<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -i 查找时不区分字符串的大小写</span></span><br><span class="line"><span class="comment"># -e 查找时使用正则表达式，匹配符合的字符串</span></span><br><span class="line"><span class="comment"># -v 查找不匹配指定字符串的行</span></span><br><span class="line"><span class="comment"># -n 查找时显示被查找字符串所在的行数</span></span><br></pre></td></tr></table></figure></li>
<li>xargs</li>
</ol>
<p>xargs要处理的文件如果不是在结尾，需要加上-i这个参数。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -n num 后面加次数，表示命令在执行的时候一次用的argument的个数，默认是用所有的</span></span><br><span class="line">cat autologin.py | xargs -n 3</span><br><span class="line"><span class="comment"># -I &#123;&#125;批定了替换字符串，表示文件内容，能循环按要求替换相应的参数  使用-I指定一个替换字符串&#123;&#125;，这个字符串在xargs扩展时会被替换掉，当-I与xargs结合使用，每一个参数命令都会被执行一次：</span></span><br><span class="line"><span class="comment"># 使用-i参数默认的前面输出用&#123;&#125;代替，-I参数可以指定其他代替字符，如例子中的[]</span></span><br><span class="line">find /var/ -name <span class="string">&quot;*.log&quot;</span> |xargs -I [] cp [] /tmp/ 【xargs 默认用是i表示&#123;&#125;，用I可以替换符号】</span><br><span class="line"><span class="comment"># -p参数会提示让你确认是否执行后面的命令,y执行，n不执行</span></span><br></pre></td></tr></table></figure></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.cnblogs.com/ftl1012/p/9250438.html">Linux xargs命令详解</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>程序端口占用问题解决方法</title>
    <url>/archives/61dac006.html</url>
    <content><![CDATA[<p>解决程序端口占用问题,使用lsof、netstat<br><a id="more"></a></p>
<h1 id="ubuntu"><a href="#ubuntu" class="headerlink" title="ubuntu"></a>ubuntu</h1><ol>
<li>lsof<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">lsof -i:端口号</span><br><span class="line"><span class="comment"># aCOMMAND   PID USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME</span></span><br><span class="line"><span class="comment"># java    13730 root  533u  IPv6 3030090      0t0  TCP localhost:7474 (LISTEN)</span></span><br></pre></td></tr></table></figure>
<h1 id="win"><a href="#win" class="headerlink" title="win"></a>win</h1></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 找到对应进程id</span></span><br><span class="line">netstat -ano |findstr <span class="string">&quot;端口号&quot;</span></span><br><span class="line"><span class="comment"># 找到对应进程名称</span></span><br><span class="line">tasklist |findstr <span class="string">&quot;进程id号&quot;</span></span><br><span class="line"><span class="comment"># 通过命令杀掉进程，或者是直接根据进程的名称杀掉所有的进程</span></span><br><span class="line">taskkill /f /t /im <span class="string">&quot;进程id或者进程名称&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
  </entry>
</search>
